{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offens_2020_eng_task1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YBgQ3I3AHcvu",
        "Cz4aVhcvvb4z",
        "GDvTes-Kn62L"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi29jeRMxM_s",
        "colab_type": "code",
        "outputId": "7d29e645-3989-45e8-df24-d2a932f73bba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MESIg6zsxc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "d750a3c8-4358-4339-d9c2-b991d5588939"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VTkjWBstxcf9",
        "outputId": "eaae5005-0a8b-4562-f9e9-10377d6cc8fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import io\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Conv1D, Conv2D\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
        "from keras.layers import average\n",
        "import tensorflow_hub as hub\n",
        "from keras.layers import Average\n",
        "from keras.layers import Concatenate\n",
        "nltk.download('punkt')\n",
        "from numpy import random\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "from keras.layers import SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "#plt.switch_backend('agg')\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzK80S4F7DdF",
        "colab_type": "code",
        "outputId": "456d55d2-01b2-4638-cdd6-1e826e6743f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install keras_metrics\n",
        "import keras_metrics as km"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.2.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.17.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE0aRShnt26Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_data=pd.read_csv('/content/drive/My Drive/offenseval/OLIDv1.0/testset-levela.tsv',delimiter='\\t')\n",
        "test_data=pd.read_csv('/content/drive/My Drive/offenseval/dev_kafi_clean_1.csv')\n",
        "labels_levels=pd.read_csv('/content/drive/My Drive/offenseval/OLIDv1.0/labels-levela.csv',header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRcYiWz-7JZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path='/content/drive/My Drive/offenseval/2020'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgCM3hXRZaH5",
        "colab_type": "code",
        "outputId": "8c71920e-239c-4153-d31a-5b8024fa3c70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "print(test_data.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id  ...                                              clean\n",
            "0  15923  ...  wholism pumplike democrats support antifat mus...\n",
            "1  27014  ...  constitutionary is revered by conservatives ha...\n",
            "2  30530  ...  forenews nra maga potus trump num nonamendment...\n",
            "3  13876  ...  watching boomer getting the news that she is s...\n",
            "4  60133  ...  unity demo to oppose the far i- right in londo...\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs_8bCmn7N9f",
        "colab_type": "code",
        "outputId": "1621e5f8-02ff-426d-fe76-ac4b0574914d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "list_df=os.listdir(root_path+\"/chunky\")\n",
        "print(list_df)\n",
        "print(len(list_df))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['data_47', 'data_54', 'data_48', 'data_55', 'data_49', 'data_56', 'data_57', 'data_100', 'data_101', 'data_102', 'data_103', 'data_10', 'data_104', 'data_11', 'data_105', 'data_12', 'data_107', 'data_13', 'data_108', 'data_109', 'data_14', 'data_110', 'data_15', 'data_111', 'data_16', 'data_112', 'data_17', 'data_113', 'data_114', 'data_18', 'data_115', 'data_19', 'data_116', 'data_20', 'data_117', 'data_21', 'data_118', 'data_119', 'data_23', 'data_59', 'data_24', 'data_60', 'data_25', 'data_61', 'data_26', 'data_62', 'data_27', 'data_63', 'data_64', 'data_28', 'data_65', 'data_29', 'data_66', 'data_30', 'data_67', 'data_31', 'data_68', 'data_69', 'data_32', 'data_70', 'data_33', 'data_71', 'data_34', 'data_72', 'data_35', 'data_73', 'data_74', 'data_36', 'data_75', 'data_37', 'data_76', 'data_38', 'data_77', 'data_39', 'data_78', 'data_40', 'data_79', 'data_80', 'data_41', 'data_82', 'data_42', 'data_83', 'data_43', 'data_84', 'data_50', 'data_85', 'data_44', 'data_86', 'data_87', 'data_45', 'data_88', 'data_51', 'data_89', 'data_52', 'data_90', 'data_91', 'data_46', 'data_92', 'data_53', 'data_93', 'data_94', 'data_96', 'data_97', 'data_98', 'data_99', 'data_0', 'data_1', 'data_2', 'data_3', 'data_4', 'data_5', 'data_6', 'data_7', 'data_8', 'data_9']\n",
            "115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzKDh5Zbj79A",
        "colab_type": "code",
        "outputId": "5eacc580-756e-4d62-95dc-f3e901ed6f53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "df_kaafi_clean=pd.read_csv('/content/drive/My Drive/offenseval/2020/kafi_clean_1.csv')\n",
        "print(df_kaafi_clean.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      label                                              clean\n",
            "0  0.305954  user too much thoughts inside his headed we ca...\n",
            "1  0.194293  first time i heard his name in camp he seems t...\n",
            "2  0.295330  when i go to drink with tumbaki he would alway...\n",
            "3  0.833349                       user his ass need to stay up\n",
            "4  0.564527  most important tweet of the day fuck donald tr...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG_eKU9FkUfQ",
        "colab_type": "code",
        "outputId": "59eb1d48-6019-4af8-a773-228f61d7cb2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_kaafi_clean.columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['label', 'clean'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57mQRFpzkX67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_kaafi_clean.drop(['text','tokens'],axis=1,inplace=True)\n",
        "# df_kaafi_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT7s-ELLkj27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_kaafi_clean.to_csv('/content/drive/My Drive/offenseval/2020/kafi_clean_1.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-EgYbr-98VZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_dummy=pd.read_csv(root_path+\"/chunky/data_0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "911He1ix_Xc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for file_name in list_df:\n",
        "#   if file_name != 'data_0':\n",
        "#     df_dummy=df_dummy.append(pd.read_csv(root_path+\"/chunky/\"+file_name),ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t66g9pD-DWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_dummy.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHxggQHy7a70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from zipfile import ZipFile\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-oZOquV85D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file_name = root_path+\"/task_a_distant.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHgBv7oj9d6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with ZipFile(file_name,'r') as zip:\n",
        "#   zip.printdir()\n",
        "#   zip.extractall(root_path,pwd=b'sem2020-t12')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TBcQvff9sqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_data=pd.read_csv('/content/drive/My Drive/offenseval/2020/task_a_distant.tsv',delimiter='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w36B594KIomg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#labels=(data['average']>0.5).astype(int)\n",
        "#print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9aUHs-P2eeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_chunky=np.array_split(train_data,120)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyEHBcqAJiap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# max_1=0\n",
        "# for i in range(120):\n",
        "#   max_1=max(df_chunky[1].shape[0],max_1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN4ZDA9hIan0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgnJfpX8Ib88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del(train_data)\n",
        "# gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBgQ3I3AHcvu",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_crFrtSHgKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus(object):\n",
        "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        #corpus_path = datapath('lee_background.cor')\n",
        "        for line in df_dummy['text']:\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nDhbpFoHgIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim.models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-kSiraHBJp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sentences = MyCorpus()\n",
        "model = gensim.models.Word2Vec(sentences=sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpLqWsOLHgG7",
        "colab_type": "code",
        "outputId": "01106c54-3ab2-4e54-c646-f05ebfce5ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile(prefix='/content/drive/My Drive/offenseval/2020/gensim-model-', delete=False) as tmp:\n",
        "    temporary_filepath = tmp.name\n",
        "    model.save(temporary_filepath)\n",
        "    #\n",
        "    # The model is now safely stored in the filepath.\n",
        "    # You can copy it to other machines, share it with others, etc.\n",
        "    #\n",
        "    # To load a saved model:"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE71pttjQZcO",
        "colab_type": "code",
        "outputId": "6325915e-1e6a-4fa5-a429-ecfda77d53c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#temporary_filepath"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/offenseval/2020/gensim-model-prnr61dj'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UqiUcPNHgCh",
        "colab_type": "code",
        "outputId": "96d3e3d2-77c5-41fe-9b32-36b095502e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "print(model.most_similar(positive=['hell'], topn=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('heck', 0.7090754508972168), ('fuck', 0.6941396594047546), ('fuckk', 0.6069889068603516), ('frick', 0.5625196099281311), ('fck', 0.5210990309715271)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnsFjsLXnv8T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mfWGtVEQuc8",
        "colab_type": "code",
        "outputId": "89d6bf4a-5c29-4687-eaab-3ca35233a069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "model.wv.save_word2vec_format('/content/drive/My Drive/offenseval/2020/twitter_trained_emb.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOcjEgBJAVOJ",
        "colab_type": "code",
        "outputId": "a4917b06-2b5b-4cbd-ded9-85dac6ef14c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/offenseval/2020/twitter_trained_emb.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4m-I9yDmuJN",
        "colab_type": "text"
      },
      "source": [
        "## Twitter embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TGj5a5ylxOB",
        "colab_type": "code",
        "outputId": "089135ae-c294-448e-f981-623557d6be8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Sentimix/word2vec_twitter_tokens.bin', binary=True,unicode_errors='ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD-JkrV5m0Vu",
        "colab_type": "code",
        "outputId": "f7d829ac-2c50-474f-88b4-231ac42e6c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model['fuck'].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz4aVhcvvb4z",
        "colab_type": "text"
      },
      "source": [
        "## Levenstein Correcter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6EBuTt2vgrJ",
        "colab_type": "code",
        "outputId": "1367fa5f-3e59-44ed-b7da-e03ccb6cf76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "import re, random\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "to_sample = False # if you're impatient switch this flag\n",
        "\n",
        "def spacy_tokenize(text):\n",
        "    return [token.text for token in nlp.tokenizer(text)]\n",
        "    \n",
        "def dameraulevenshtein(seq1, seq2):\n",
        "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
        "    This method has not been modified from the original.\n",
        "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
        "    This distance is the number of additions, deletions, substitutions,\n",
        "    and transpositions needed to transform the first sequence into the\n",
        "    second. Although generally used with strings, any sequences of\n",
        "    comparable objects will work.\n",
        "    Transpositions are exchanges of *consecutive* characters; all other\n",
        "    operations are self-explanatory.\n",
        "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
        "    lengths of the two sequences.\n",
        "    >>> dameraulevenshtein('ba', 'abc')\n",
        "    2\n",
        "    >>> dameraulevenshtein('fee', 'deed')\n",
        "    2\n",
        "    It works with arbitrary sequences too:\n",
        "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
        "    2\n",
        "    \"\"\"\n",
        "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
        "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
        "    # However, only the current and two previous rows are needed at once,\n",
        "    # so we only store those.\n",
        "    oneago = None\n",
        "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
        "    for x in range(len(seq1)):\n",
        "        # Python lists wrap around for negative indices, so put the\n",
        "        # leftmost column at the *end* of the list. This matches with\n",
        "        # the zero-indexed strings and saves extra calculation.\n",
        "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
        "        for y in range(len(seq2)):\n",
        "            delcost = oneago[y] + 1\n",
        "            addcost = thisrow[y - 1] + 1\n",
        "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
        "            thisrow[y] = min(delcost, addcost, subcost)\n",
        "            # This block deals with transpositions\n",
        "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
        "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
        "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
        "    return thisrow[len(seq2) - 1]\n",
        "\n",
        "\n",
        "class SymSpell:\n",
        "    def __init__(self, max_edit_distance=3, verbose=0):\n",
        "        self.max_edit_distance = max_edit_distance\n",
        "        self.verbose = verbose\n",
        "        # 0: top suggestion\n",
        "        # 1: all suggestions of smallest edit distance\n",
        "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
        "\n",
        "        self.dictionary = {}\n",
        "        self.longest_word_length = 0\n",
        "\n",
        "    def get_deletes_list(self, w):\n",
        "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
        "           deleted\"\"\"\n",
        "\n",
        "        deletes = []\n",
        "        queue = [w]\n",
        "        for d in range(self.max_edit_distance):\n",
        "            temp_queue = []\n",
        "            for word in queue:\n",
        "                if len(word) > 1:\n",
        "                    for c in range(len(word)):  # character index\n",
        "                        word_minus_c = word[:c] + word[c + 1:]\n",
        "                        if word_minus_c not in deletes:\n",
        "                            deletes.append(word_minus_c)\n",
        "                        if word_minus_c not in temp_queue:\n",
        "                            temp_queue.append(word_minus_c)\n",
        "            queue = temp_queue\n",
        "\n",
        "        return deletes\n",
        "\n",
        "    def create_dictionary_entry(self, w):\n",
        "        '''add word and its derived deletions to dictionary'''\n",
        "        # check if word is already in dictionary\n",
        "        # dictionary entries are in the form: (list of suggested corrections,\n",
        "        # frequency of word in corpus)\n",
        "        new_real_word_added = False\n",
        "        if w in self.dictionary:\n",
        "            # increment count of word in corpus\n",
        "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
        "        else:\n",
        "            self.dictionary[w] = ([], 1)\n",
        "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
        "\n",
        "        if self.dictionary[w][1] == 1:\n",
        "            # first appearance of word in corpus\n",
        "            # n.b. word may already be in dictionary as a derived word\n",
        "            # (deleting character from a real word)\n",
        "            # but counter of frequency of word in corpus is not incremented\n",
        "            # in those cases)\n",
        "            new_real_word_added = True\n",
        "            deletes = self.get_deletes_list(w)\n",
        "            for item in deletes:\n",
        "                if item in self.dictionary:\n",
        "                    # add (correct) word to delete's suggested correction list\n",
        "                    self.dictionary[item][0].append(w)\n",
        "                else:\n",
        "                    # note frequency of word in corpus is not incremented\n",
        "                    self.dictionary[item] = ([w], 0)\n",
        "\n",
        "        return new_real_word_added\n",
        "\n",
        "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        for line in arr:\n",
        "            # separate by words by non-alphabetical characters\n",
        "            words = re.findall(token_pattern, line.lower())\n",
        "            for word in words:\n",
        "                total_word_count += 1\n",
        "                if self.create_dictionary_entry(word):\n",
        "                    unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def create_dictionary(self, fname):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        with open(fname) as file:\n",
        "            for line in file:\n",
        "                # separate by words by non-alphabetical characters\n",
        "                words = re.findall('[a-z]+', line.lower())\n",
        "                for word in words:\n",
        "                    total_word_count += 1\n",
        "                    if self.create_dictionary_entry(word):\n",
        "                        unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def get_suggestions(self, string, silent=False):\n",
        "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
        "           spelled word\"\"\"\n",
        "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
        "            if not silent:\n",
        "                print(\"no items in dictionary within maximum edit distance\")\n",
        "            return []\n",
        "\n",
        "        suggest_dict = {}\n",
        "        min_suggest_len = float('inf')\n",
        "\n",
        "        queue = [string]\n",
        "        q_dictionary = {}  # items other than string that we've checked\n",
        "\n",
        "        while len(queue) > 0:\n",
        "            q_item = queue[0]  # pop\n",
        "            queue = queue[1:]\n",
        "\n",
        "            # early exit\n",
        "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
        "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
        "                break\n",
        "\n",
        "            # process queue item\n",
        "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
        "                if self.dictionary[q_item][1] > 0:\n",
        "                    # word is in dictionary, and is a word from the corpus, and\n",
        "                    # not already in suggestion list so add to suggestion\n",
        "                    # dictionary, indexed by the word with value (frequency in\n",
        "                    # corpus, edit distance)\n",
        "                    # note q_items that are not the input string are shorter\n",
        "                    # than input string since only deletes are added (unless\n",
        "                    # manual dictionary corrections are added)\n",
        "                    assert len(string) >= len(q_item)\n",
        "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
        "                                            len(string) - len(q_item))\n",
        "                    # early exit\n",
        "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
        "                        break\n",
        "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
        "                        min_suggest_len = len(string) - len(q_item)\n",
        "\n",
        "                # the suggested corrections for q_item as stored in\n",
        "                # dictionary (whether or not q_item itself is a valid word\n",
        "                # or merely a delete) can be valid corrections\n",
        "                for sc_item in self.dictionary[q_item][0]:\n",
        "                    if sc_item not in suggest_dict:\n",
        "\n",
        "                        # compute edit distance\n",
        "                        # suggested items should always be longer\n",
        "                        # (unless manual corrections are added)\n",
        "                        assert len(sc_item) > len(q_item)\n",
        "\n",
        "                        # q_items that are not input should be shorter\n",
        "                        # than original string\n",
        "                        # (unless manual corrections added)\n",
        "                        assert len(q_item) <= len(string)\n",
        "\n",
        "                        if len(q_item) == len(string):\n",
        "                            assert q_item == string\n",
        "                            item_dist = len(sc_item) - len(q_item)\n",
        "\n",
        "                        # item in suggestions list should not be the same as\n",
        "                        # the string itself\n",
        "                        assert sc_item != string\n",
        "\n",
        "                        # calculate edit distance using, for example,\n",
        "                        # Damerau-Levenshtein distance\n",
        "                        item_dist = dameraulevenshtein(sc_item, string)\n",
        "\n",
        "                        # do not add words with greater edit distance if\n",
        "                        # verbose setting not on\n",
        "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
        "                            pass\n",
        "                        elif item_dist <= self.max_edit_distance:\n",
        "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
        "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
        "                            if item_dist < min_suggest_len:\n",
        "                                min_suggest_len = item_dist\n",
        "\n",
        "                        # depending on order words are processed, some words\n",
        "                        # with different edit distances may be entered into\n",
        "                        # suggestions; trim suggestion dictionary if verbose\n",
        "                        # setting not on\n",
        "                        if self.verbose < 2:\n",
        "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
        "\n",
        "            # now generate deletes (e.g. a substring of string or of a delete)\n",
        "            # from the queue item\n",
        "            # as additional items to check -- add to end of queue\n",
        "            assert len(string) >= len(q_item)\n",
        "\n",
        "            # do not add words with greater edit distance if verbose setting\n",
        "            # is not on\n",
        "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
        "                pass\n",
        "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
        "                for c in range(len(q_item)):  # character index\n",
        "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
        "                    if word_minus_c not in q_dictionary:\n",
        "                        queue.append(word_minus_c)\n",
        "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
        "\n",
        "        # queue is now empty: convert suggestions in dictionary to\n",
        "        # list for output\n",
        "        if not silent and self.verbose != 0:\n",
        "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
        "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "\n",
        "        # output option 1\n",
        "        # sort results by ascending order of edit distance and descending\n",
        "        # order of frequency\n",
        "        #     and return list of suggested word corrections only:\n",
        "        # return sorted(suggest_dict, key = lambda x:\n",
        "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
        "\n",
        "        # output option 2\n",
        "        # return list of suggestions with (correction,\n",
        "        #                                  (frequency in corpus, edit distance)):\n",
        "        as_list = suggest_dict.items()\n",
        "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
        "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
        "\n",
        "        if self.verbose == 0:\n",
        "            return outlist[0]\n",
        "        else:\n",
        "            return outlist\n",
        "\n",
        "        '''\n",
        "        Option 1:\n",
        "        ['file', 'five', 'fire', 'fine', ...]\n",
        "        Option 2:\n",
        "        [('file', (5, 0)),\n",
        "         ('five', (67, 1)),\n",
        "         ('fire', (54, 1)),\n",
        "         ('fine', (17, 1))...]  \n",
        "        '''\n",
        "\n",
        "    def best_word(self, s, silent=False):\n",
        "        try:\n",
        "            return self.get_suggestions(s, silent)[0]\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def spell_corrector(word_list, words_d) -> str:\n",
        "    result_list = []\n",
        "    for word in word_list:\n",
        "        if word not in words_d:\n",
        "            suggestion = ss.best_word(word, silent=True)\n",
        "            if suggestion is not None:\n",
        "                result_list.append(suggestion)\n",
        "        else:\n",
        "            result_list.append(word)\n",
        "            \n",
        "    return \" \".join(result_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # build symspell tree \n",
        "    ss = SymSpell(max_edit_distance=2)\n",
        "    \n",
        "    # fetch list of bad words\n",
        "    with open('/content/drive/My Drive/offenseval/2020/cleaner_data/bad-words.csv') as bf:\n",
        "        bad_words = bf.readlines()\n",
        "    bad_words = [word.strip() for word in bad_words]    \n",
        "    \n",
        "    # fetch english words dictionary\n",
        "    with open('/content/drive/My Drive/offenseval/2020/cleaner_data/english_words_479k.txt') as f:\n",
        "        words = f.readlines()\n",
        "    eng_words = [word.strip() for word in words]\n",
        "    \n",
        "    # Print some examples\n",
        "    print(eng_words[:5])\n",
        "    print(bad_words[:5])\n",
        "\n",
        "    print('Total english words: {}'.format(len(eng_words)))\n",
        "    print('Total bad words: {}'.format(len(bad_words)))\n",
        "    \n",
        "    print('create symspell dict...')\n",
        "    \n",
        "    if to_sample:\n",
        "        # sampling from list for kernel runtime\n",
        "        sample_idxs = random.sample(range(len(eng_words)), 100)\n",
        "        eng_words = [eng_words[i] for i in sorted(sample_idxs)] + \\\n",
        "            'to infinity and beyond'.split() # make sure our sample misspell is in there\n",
        "    \n",
        "    all_words_list = list(set(bad_words + eng_words))\n",
        "    silence = ss.create_dictionary_from_arr(all_words_list, token_pattern=r'.+')\n",
        "    \n",
        "    # create a dictionary of rightly spelled words for lookup\n",
        "    words_dict = {k: 0 for k in all_words_list}\n",
        "    \n",
        "    sample_text = 'to infifity and byond'\n",
        "    test_data['tokens'] = test_data['text'].apply(spacy_tokenize)\n",
        "    \n",
        "    print('run spell checker...')\n",
        "    print()\n",
        "    #print('original text: ' + sample_text)\n",
        "    print()\n",
        "    test_data['clean'] = test_data.apply(lambda row:spell_corrector(row['tokens'],words_dict),axis=1)\n",
        "    #print('corrected text: ' + correct_text)\n",
        "\n",
        "    print('Done.')    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2', '1080', '&c', '10-point', '10th']\n",
            "['jigaboo', 'mound of venus', 'asslover', 's&m', 'queaf']\n",
            "Total english words: 466544\n",
            "Total bad words: 1617\n",
            "create symspell dict...\n",
            "total words processed: 467594\n",
            "total unique words in corpus: 467394\n",
            "total items in dictionary (corpus words and deletions): 20250415\n",
            "  edit distance for deletions: 2\n",
            "  length of longest word in corpus: 45\n",
            "run spell checker...\n",
            "\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOYLYI3hkrnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data.to_csv('/content/drive/My Drive/offenseval/dev_kafi_clean_1.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDvTes-Kn62L",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8t7HZzvFwe",
        "colab_type": "code",
        "outputId": "62bb2615-902b-4963-e8c3-307716155d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l0lIDviExdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1NH5q2QnTq5",
        "colab_type": "code",
        "outputId": "1e61d732-fa51-4b6d-e156-89556b4cfa0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "def remove_pattern(input_txt, pattern,with_space=False):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    if with_space==False:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    else:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, ' ', input_txt)\n",
        "    return input_txt \n",
        "!pip install emoji\n",
        "import emoji\n",
        "import pickle\n",
        "import re\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/contractions.pkl','rb')as f:\n",
        "  contractions=pickle.load(f)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "contractions=Counter(contractions)\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/acronyms.pkl','rb')as f:\n",
        "  acronyms=pickle.load(f)\n",
        "acronyms=Counter(acronyms)\n",
        "def acronym(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if acronyms[word]!=0:\n",
        "        w_l.append(acronyms[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "# with open('/content/drive/My Drive/Sentimix/hinglish_to_english.pickle','rb')as f:\n",
        "#   hing_to_eng=pickle.load(f)\n",
        "# hing_to_eng=Counter(hing_to_eng)\n",
        "def hindi_se_english(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    w_l=[]\n",
        "    sent=str(df[column][i])\n",
        "    for word in sent.split():\n",
        "      if hing_to_eng[word]!=0:\n",
        "        w_l.append(hing_to_eng[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "# with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
        "#     cuss_dict=pickle.load(handle)\n",
        "# cuss_dict=Counter(cuss_dict)\n",
        "def replace_cuss(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if cuss_dict[word]!=0:\n",
        "        w_l.append('abuse')\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_contraction(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if contractions[word]!=0:\n",
        "        w_l.append(contractions[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_pattern_rep(input_txt, pattern,rep_pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "      input_txt = re.sub(i, rep_pattern, input_txt)\n",
        "\n",
        "    return input_txt \n",
        "def cleaning(data_f,cleaning_col,new_col):\n",
        "  data_f.reset_index(drop=True,inplace=True)\n",
        "  for i in tqdm(range(data_f.shape[0])):\n",
        "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
        "  #data_f[cleaning_col]=replace_cuss(data_f,cleaning_col)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[cleaning_col],\"_\",with_space=True)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@[\\w]*\",\"<USR>\")\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"http\\S+\",\"<URL>\")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[cleaning_col], \"[0-9]+\",\"<NUM>\")\n",
        "  #data_f[new_col]=hindi_se_english(data_f,cleaning_col)\n",
        "  data_f[new_col]=remove_contraction(data_f,new_col)\n",
        "  data_f[new_col]=acronym(data_f,new_col)\n",
        "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
        "  #data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "  return data_f\n",
        "import numpy as np\n",
        "#a=cleaning(data,'text','clean_col')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=ee466fcf0703f783189e48d97795cfe2ffbcad9801d014ddad3119b08a30eab9\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVC7JZXSfuJN",
        "colab_type": "code",
        "outputId": "2a16d731-d0b9-4f4e-995f-9bba1c5d4d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "test_data=cleaning(test_data,'tweet','clean_col')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/860 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "100%|██████████| 860/860 [00:00<00:00, 1427.58it/s]\n",
            "100%|██████████| 860/860 [00:00<00:00, 48811.90it/s]\n",
            "100%|██████████| 860/860 [00:00<00:00, 48522.33it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p3kwFJpJHtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_chunky[18].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTiApGzZ_HKd",
        "colab_type": "code",
        "outputId": "de736cce-5ed3-4cb1-8204-11a1c8dcc6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyb_RqvmBkZQ",
        "colab_type": "code",
        "outputId": "6edaadff-2d3e-4de5-933a-c15e6be046d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker(distance=1)\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['fuckk'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print(spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print(spell.candidates(word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuck\n",
            "{'fuck', 'fucks'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFNwTOxHxK6F",
        "colab_type": "code",
        "outputId": "b5469f74-aa17-41fa-9813-f9c7040e2535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spell.correction(\"tt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S5hphZPwOmQ",
        "colab_type": "code",
        "outputId": "098df384-1cce-4cfa-e78b-5bbd28cdf533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a=\"how are you @user\"\n",
        "a.split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how', 'are', 'you', '@user']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMNj2_kEwzTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spell_check(df):\n",
        "  for i in range(df.shape[0]):\n",
        "    if i%100000==0:\n",
        "      print(\"Reached {0}, percent {1}\".format(i,float(i/df.shape[0])*100))\n",
        "    words=df['text'][i].split()\n",
        "    misspelled = spell.unknown(words)\n",
        "    l=[]\n",
        "    for word in words:\n",
        "      if word in misspelled:\n",
        "        word=spell.correction(word)\n",
        "      else:\n",
        "        word=word\n",
        "      l.append(word)\n",
        "    #words=[spell.correction(word) for word in words]\n",
        "    df['text'][i]=' '.join(word for word in l)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_SWa1EexlXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#spell_check(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jZWcngfJFWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spell_collection(df):\n",
        "  spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', df)\n",
        "  return spell_corrected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIAweYarr2jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_check=pd.read_csv(root_path+\"/chunky/data_0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7x1KJasgN0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data['text']=test_data['clean_col'].apply(spell_collection)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDoS4fOpgu-w",
        "colab_type": "code",
        "outputId": "9586a29f-f57c-4bfe-b140-ccd309d461bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "test_data=spell_check(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reached 0, percent 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzn2DxutvHSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_check['text']=df_check['text'].apply(spell_collection)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p0IoA8Mxy0i",
        "colab_type": "code",
        "outputId": "78e3a9c0-4ef2-4c83-f9b6-f87284f93da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "%%time\n",
        "#df_check['text']=df_check['text'].apply(spell_check)\n",
        "spell_check(df_check)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 14s, sys: 96.4 ms, total: 3min 14s\n",
            "Wall time: 3min 14s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>user too much thoughts inside his head we cann...</td>\n",
              "      <td>0.305954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>first time i heard his name in camp he seems t...</td>\n",
              "      <td>0.194293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>when i go to drink with tsunami he would alway...</td>\n",
              "      <td>0.295330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>user his ass need to stay up :face_with_tears_...</td>\n",
              "      <td>0.833349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>most important tweet of the day : fuck donald ...</td>\n",
              "      <td>0.564527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75607</th>\n",
              "      <td>a paranoid is someone who knows a little of wh...</td>\n",
              "      <td>0.494083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75608</th>\n",
              "      <td>blue wants the world to know that it is unfair...</td>\n",
              "      <td>0.314968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75609</th>\n",
              "      <td>the crowd noise is so realistic makes you feel...</td>\n",
              "      <td>0.267466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75610</th>\n",
              "      <td>once you meet your breaking point dont worry h...</td>\n",
              "      <td>0.186014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75611</th>\n",
              "      <td>user user so happy for the video she is truly ...</td>\n",
              "      <td>0.153173</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75612 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text     label\n",
              "0      user too much thoughts inside his head we cann...  0.305954\n",
              "1      first time i heard his name in camp he seems t...  0.194293\n",
              "2      when i go to drink with tsunami he would alway...  0.295330\n",
              "3      user his ass need to stay up :face_with_tears_...  0.833349\n",
              "4      most important tweet of the day : fuck donald ...  0.564527\n",
              "...                                                  ...       ...\n",
              "75607  a paranoid is someone who knows a little of wh...  0.494083\n",
              "75608  blue wants the world to know that it is unfair...  0.314968\n",
              "75609  the crowd noise is so realistic makes you feel...  0.267466\n",
              "75610  once you meet your breaking point dont worry h...  0.186014\n",
              "75611  user user so happy for the video she is truly ...  0.153173\n",
              "\n",
              "[75612 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIKTSjeovX3U",
        "colab_type": "code",
        "outputId": "3ed710e7-eacc-4780-c48b-1507617e740a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_check['text'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'@user too much thoughts inside his headdd we cannot even imagine tt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLvRuPD3_zIP",
        "colab_type": "code",
        "outputId": "fbd1d249-b048-47b1-b2ad-ec1bc1a8fa34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(df_dummy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17315066"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqLO6F1-Fb9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dummy['text']=df_dummy['text'].apply(spell_collection)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvyInQftJDhN",
        "colab_type": "code",
        "outputId": "b5bd6953-a7f6-46fb-fb11-7315069a4c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_dummy['text'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'user too much thoughts inside his headed we cannon even imagine tt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_otxK0IJwIk",
        "colab_type": "code",
        "outputId": "3cb0f5d1-b7c9-461f-d3e8-af86baef6a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "for file_name in list_df[0:10]:\n",
        "  #if file_name != 'data_0':\n",
        "    #df_dummy=df_dummy.append(pd.read_csv(root_path+\"/chunky/\"+file_name),ignore_index=True)\n",
        "  df_x=pd.read_csv(root_path+\"/chunky/\"+file_name)\n",
        "  df_x['text']=df_x['text'].apply(spell_collection)\n",
        "  print(file_name)\n",
        "  df_x=spell_check(df_x)\n",
        "  df_x.to_csv(root_path+\"/chunky/\"+file_name,index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_0\n",
            "Reached 0, percent 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data_1\n",
            "Reached 0, percent 0.0\n",
            "data_2\n",
            "Reached 0, percent 0.0\n",
            "data_3\n",
            "Reached 0, percent 0.0\n",
            "data_4\n",
            "Reached 0, percent 0.0\n",
            "data_5\n",
            "Reached 0, percent 0.0\n",
            "data_6\n",
            "Reached 0, percent 0.0\n",
            "data_7\n",
            "Reached 0, percent 0.0\n",
            "data_8\n",
            "Reached 0, percent 0.0\n",
            "data_9\n",
            "Reached 0, percent 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHIF_mARvf0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_dummy=spell_check(df_dummy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp_USuba26I_",
        "colab_type": "code",
        "outputId": "692db3c8-d3cb-40c1-ad73-840df90c4cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# for i in range(96,100):\n",
        "#   print(i)\n",
        "#   a=cleaning(df_chunky[i],'text','clean_col')\n",
        "#   df=pd.DataFrame({'text':a['clean_col'],'label':a['average']})\n",
        "#   df.to_csv('/content/drive/My Drive/offenseval/2020/chunky/data_'+str(i),index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/75611 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  0%|          | 75/75611 [00:00<01:41, 743.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75611/75611 [00:56<00:00, 1344.38it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 61150.02it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 59468.28it/s]\n",
            "  0%|          | 131/75611 [00:00<00:57, 1303.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "97\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75611/75611 [00:57<00:00, 1316.93it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 60547.19it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 58265.06it/s]\n",
            "  0%|          | 135/75611 [00:00<00:56, 1346.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "98\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75611/75611 [00:57<00:00, 1306.09it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 59774.23it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 57725.33it/s]\n",
            "  0%|          | 132/75611 [00:00<00:57, 1319.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "99\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75611/75611 [00:56<00:00, 1329.55it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 59982.76it/s]\n",
            "100%|██████████| 75611/75611 [00:01<00:00, 58537.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdajs4zjqiF7",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Train test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cUy9gOioPdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGOZ-FwXq1-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_train,df_dev=train_test_split(df_kaafi_clean,test_size=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn1VnVvWdE8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzfYrErFBWN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=(df_train['label']>=0.45).astype(int)\n",
        "y_dev=(df_dev['label']>0.45).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr_1CJL9dYj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_levels['label']=(labels_levels[1]=='OFF').astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vdKhyWhd1Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test=labels_levels['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHeKckLprUHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_data=pd.DataFrame({'text':a['clean_col'],'labels':labels})\n",
        "# df_data.to_csv(root_path+\"/clean_eng_task_1.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEhsipVdB_Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40smU_LTHaS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words =50000\n",
        "max_len = 35\n",
        "tok = Tokenizer(max_words)\n",
        "tok.fit_on_texts(df_kaafi_clean['clean'].astype(str))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTvcgJDxISzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_train = tok.texts_to_sequences(df_train['clean'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbvCd_GDIb9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_dev = tok.texts_to_sequences(df_dev['clean'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6qE9ejld-6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_test = tok.texts_to_sequences(test_data['clean'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AlHf383IfzQ",
        "colab_type": "code",
        "outputId": "4ec88ca7-9c89-475a-f4e7-af5b35bc175b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_S9zYeaIjCZ",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB_DuGFzIiM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/drive/My Drive/IR_project/glove.6B', 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnExMh9BIrIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index=tok.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BjfK1znnm-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_matrix_1 = np.zeros([max_words + 1, 300])\n",
        "# for word, i in tok.word_index.items():\n",
        "#     if word in embeddings_index.keys():\n",
        "#       if i>max_words:\n",
        "#         break\n",
        "#       embedding_matrix_1[i] = embeddings_index[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgYbissiIs43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix_1 = np.zeros([max_words + 1, 400])\n",
        "for word, i in tok.word_index.items():\n",
        "    if word in model:\n",
        "      if i>max_words:\n",
        "        break\n",
        "      embedding_matrix_1[i] = model[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ow_LVZCIugP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62rVZCNKIwOx",
        "colab_type": "code",
        "outputId": "5731d19c-42e6-4227-c587-bd0c7f0eaf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from sklearn.utils import class_weight\n",
        "class Attention(Layer):\n",
        "    def __init__(self,step_dim=20,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, Dropout\n",
        "\n",
        "#max_len=\n",
        "\n",
        "def BidLstm(maxlen, max_features, embed_size):\n",
        "    inp1 = Input(shape=(maxlen, ))\n",
        "    #inp2=Input(shape=(1,))\n",
        "    #x=Embedding(len(word_index)+1,embed_size)(inp1)\n",
        "    x1 = Embedding(max_words + 1,embed_size,weights=[embedding_matrix_1],\n",
        "                  trainable=True)(inp1)\n",
        "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x1)\n",
        "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x2)\n",
        "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x3)   \n",
        "    # x1 = Attention(maxlen)(x1)\n",
        "    # x2 = Attention(maxlen)(x2)\n",
        "    # x3 = Attention(maxlen)(x3)\n",
        "    # x=  Concatenate()([x1,x2,x3])\n",
        "    x1 = CuDNNLSTM(200, return_sequences=True)(x1)   \n",
        "    x2=  GlobalMaxPooling1D()(x1)\n",
        "    x3= GlobalAveragePooling1D()(x1)\n",
        "    x=  Concatenate()([x2,x3])\n",
        "    x = Dropout(0.1)(x)\n",
        "    #x = Attention(maxlen)(x)\n",
        "    # layer = Dense(600,name='FC1')(x)\n",
        "    # layer = Dense(300,activation='relu')(layer)\n",
        "    layer = Dense(128,activation='relu')(x)\n",
        " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
        "    #layer = Activation('relu')(layer)\n",
        "    #layer = Dropout(0.4)(layer)\n",
        "    layer = Dense(64,name='FC2')(layer)\n",
        "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.4)(layer)\n",
        "   # layer=  Concatenate()([layer,inp2])\n",
        "    # layer=Dense(256,activation='relu')(layer)\n",
        "    # layer=Dense(128,activation='relu')(layer)\n",
        "    layer = Dense(1,name='out_layer',activation='sigmoid')(layer)\n",
        "\n",
        "    model = Model(inputs=[inp1],outputs=layer)\n",
        "\n",
        "    return model\n",
        "model_bi=BidLstm(max_len,max_features=max_words,embed_size=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVuU-dqQI0kX",
        "colab_type": "code",
        "outputId": "9190739f-5eee-47f7-d84f-21daa77137f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "model_bi.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc',km.f1_score()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MOxZ2KlI3XQ",
        "colab_type": "code",
        "outputId": "a62a5712-a192-4b6e-c643-c9621a4f0829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "model_bi.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 35)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 35, 400)      20000400    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)        (None, 35, 200)      481600      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 200)          0           cu_dnnlstm_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 200)          0           cu_dnnlstm_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 400)          0           global_max_pooling1d_1[0][0]     \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 400)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          51328       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "FC2 (Dense)                     (None, 64)           8256        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 64)           0           FC2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 64)           0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "out_layer (Dense)               (None, 1)            65          dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 20,541,649\n",
            "Trainable params: 20,541,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unoM9G-5afxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=(df_kaafi_clean['label']>=0.45).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSjy1UG5I5C5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y),y)\n",
        "class_weights=dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHctXiJCtrej",
        "colab_type": "code",
        "outputId": "4aa3aa78-7d68-4a28-b62d-643f7bb8fbc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(class_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.6168365626311924, 1: 2.639741142412353}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ig7ivSoI7BQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_filepath='/content/drive/My Drive/offenseval/'+'checkpoints/lstm_model_2020a.h5'\n",
        "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
        "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=2,restore_best_weights=True)\n",
        "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiaFhksjI-EZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_bi.fit([sequences_matrix_train],y_train,validation_data=([sequences_matrix_dev],y_dev),epochs=10,batch_size=1024,class_weight=class_weights,callbacks=[es,cp_check_point])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKxPEZgrbDRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRfYg1SonN7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTewa1FIaBdp",
        "colab_type": "code",
        "outputId": "7f12a5bf-13b2-4da1-88d7-a8d0f49c1b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "f1s=[]\n",
        "for train_ind,dev_ind in kfold.split(df_kaafi_clean['clean'],y):\n",
        "  #print(train_data['tweet'][train_ind].shape)\n",
        "  x_train=df_kaafi_clean['clean'][train_ind]\n",
        "  y_train=y[train_ind]\n",
        "  x_dev=df_kaafi_clean['clean'][dev_ind]\n",
        "  y_dev=y[dev_ind]\n",
        "  seq_train=tok.texts_to_sequences(x_train.astype(str))\n",
        "  seq_matrix_train=sequence.pad_sequences(seq_train,maxlen=max_len,padding='post',truncating='post')\n",
        "  seq_dev=tok.texts_to_sequences(x_dev.astype(str))\n",
        "  seq_matrix_dev=sequence.pad_sequences(seq_dev,maxlen=max_len,padding='post',truncating='post')\n",
        "  model_bi.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc',km.f1_score()])\n",
        "  model_bi.fit([seq_matrix_train],y_train,validation_split=0.1,epochs=10,batch_size=2056,class_weight=class_weights,callbacks=[es,cp_check_point])\n",
        "  scores = model_bi.evaluate(seq_matrix_dev, y_dev, verbose=0)\n",
        "  pred = model_bi.predict(seq_matrix_dev)\n",
        "  print(classification_report(y_dev, pred.round()))\n",
        "  f1s.append(f1_score(y_dev,pred.round(),average='macro'))\n",
        "  #f1s.append(scores[2])\n",
        "  print(scores)\n",
        "print(\"Mean f1:{0}, standard dev:{1}\".format(np.mean(f1s),np.std(f1s)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6260643 samples, validate on 695628 samples\n",
            "Epoch 1/10\n",
            "6260643/6260643 [==============================] - 198s 32us/step - loss: 0.0834 - acc: 0.9673 - f1_score: 0.9190 - val_loss: 0.1436 - val_acc: 0.9520 - val_f1_score: 0.8812\n",
            "Epoch 2/10\n",
            "6260643/6260643 [==============================] - 199s 32us/step - loss: 0.0748 - acc: 0.9706 - f1_score: 0.9269 - val_loss: 0.1630 - val_acc: 0.9526 - val_f1_score: 0.8820\n",
            "Epoch 3/10\n",
            "6260643/6260643 [==============================] - 197s 32us/step - loss: 0.0671 - acc: 0.9737 - f1_score: 0.9341 - val_loss: 0.1783 - val_acc: 0.9498 - val_f1_score: 0.8760\n",
            "Epoch 4/10\n",
            "6260643/6260643 [==============================] - 198s 32us/step - loss: 0.0606 - acc: 0.9761 - f1_score: 0.9400 - val_loss: 0.2066 - val_acc: 0.9534 - val_f1_score: 0.8830\n",
            "Epoch 5/10\n",
            "6260643/6260643 [==============================] - 198s 32us/step - loss: 0.0551 - acc: 0.9783 - f1_score: 0.9451 - val_loss: 0.2386 - val_acc: 0.9551 - val_f1_score: 0.8856\n",
            "Epoch 6/10\n",
            "6260643/6260643 [==============================] - 198s 32us/step - loss: 0.0508 - acc: 0.9797 - f1_score: 0.9487 - val_loss: 0.2408 - val_acc: 0.9533 - val_f1_score: 0.8820\n",
            "Epoch 7/10\n",
            "6260643/6260643 [==============================] - 198s 32us/step - loss: 0.0469 - acc: 0.9812 - f1_score: 0.9524 - val_loss: 0.2541 - val_acc: 0.9536 - val_f1_score: 0.8822\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97   1409667\n",
            "           1       0.85      0.93      0.89    329401\n",
            "\n",
            "    accuracy                           0.95   1739068\n",
            "   macro avg       0.92      0.94      0.93   1739068\n",
            "weighted avg       0.96      0.95      0.96   1739068\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-98d97a592a56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_bi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_matrix_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mf1s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;31m#f1s.append(scores[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ1RAlqMnUqi",
        "colab_type": "code",
        "outputId": "5a4a2a34-d066-4a68-a777-b554e1923b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "print(classification_report(y_dev, pred.round()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97   1409667\n",
            "           1       0.84      0.94      0.89    329401\n",
            "\n",
            "    accuracy                           0.96   1739068\n",
            "   macro avg       0.91      0.95      0.93   1739068\n",
            "weighted avg       0.96      0.96      0.96   1739068\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LETjIZuJBiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_dl=model_bi.predict(sequences_matrix_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdU00r0WJvzV",
        "colab_type": "code",
        "outputId": "79560896-e325-4c9e-c720-e1e024e8c510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#y_pred = model_bi.predict(X_dev, batch_size=30, verbose=1)\n",
        "\n",
        "print(classification_report(y_test, y_preds_dl.round()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89       620\n",
            "           1       0.74      0.64      0.69       240\n",
            "\n",
            "    accuracy                           0.84       860\n",
            "   macro avg       0.81      0.78      0.79       860\n",
            "weighted avg       0.83      0.84      0.83       860\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0kz3eI_JyTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}