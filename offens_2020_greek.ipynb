{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offens_2020_greek.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asking28/offenseval2020/blob/master/offens_2020_greek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0BLcJep5y5Z",
        "colab_type": "code",
        "outputId": "e5ad83c1-f79c-4118-fada-e3cedb2b15d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8cpzl9qV0T9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05895aae-259d-4bac-ffcb-4274734f8ff0"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiGa_VYuqurM",
        "colab_type": "code",
        "outputId": "8eefb331-c006-45d4-a4ba-6c2ba26092fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install focal-loss\n",
        "!pip install keras-tcn==2.8.3\n",
        "!pip install keras-multi-head"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting focal-loss\n",
            "  Downloading https://files.pythonhosted.org/packages/66/ed/17450291228192ad8595de4514c8ec28a587697b03c707d12d4af5b7f331/focal_loss-0.0.2-py3-none-any.whl\n",
            "Installing collected packages: focal-loss\n",
            "Successfully installed focal-loss-0.0.2\n",
            "Collecting keras-tcn==2.8.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/c4/438c86b27ab11a79cc659d8d6878682c4eb80caa0c0b3d620740cef762f5/keras_tcn-2.8.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn==2.8.3) (1.18.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn==2.8.3) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.1.0)\n",
            "Installing collected packages: keras-tcn\n",
            "Successfully installed keras-tcn-2.8.3\n",
            "Collecting keras-multi-head\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (1.18.2)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (2.3.1)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.1.0)\n",
            "Building wheels for collected packages: keras-multi-head, keras-self-attention\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=2893029c8e8d91fb11aac8a7dab4c393dfa2c29eafc02e5008091b172672d21b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=8242d5e4c097fe87b5f04127f50ae58de647169f4cc2d0733bd3d8b87a29bf20\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-multi-head keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-multi-head\n",
            "Successfully installed keras-multi-head-0.22.0 keras-self-attention-0.41.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t3mWlV357Bw",
        "colab_type": "code",
        "outputId": "52c89a02-3f6d-4d3c-a066-03a3b5fd518d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import io\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Conv1D, Conv2D\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras_multi_head import MultiHeadAttention\n",
        "from keras_multi_head import MultiHead\n",
        "import nltk\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
        "from keras.layers import average\n",
        "import tensorflow_hub as hub\n",
        "from keras.layers import Average\n",
        "from keras.layers import Concatenate\n",
        "nltk.download('punkt')\n",
        "from numpy import random\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "from keras.layers import SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "from focal_loss import BinaryFocalLoss\n",
        "#plt.switch_backend('agg')\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ZRPEX16GqP",
        "colab_type": "code",
        "outputId": "100f73c5-44b3-4eca-e7fd-c4cdd3389ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install keras_metrics\n",
        "import keras_metrics as km"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.18.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.10.0)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU4vkM4u6Wln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path='/content/drive/My Drive/offenseval/2020'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrWhJIr16hwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=pd.read_csv(root_path+'/non_english_data/Greek/offenseval-greek-training-v1.tsv',delimiter='\\t',quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCHMU7req0z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data=pd.read_csv(root_path+'/non_english_data/Greek/testset_taska.tsv',delimiter='\\t',quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_SdDs7u5MSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_labels=pd.read_csv(root_path+'/non_english_data/Greek/greek-goldlabels.csv',header=None,quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ7OozNa7iOJ",
        "colab_type": "code",
        "outputId": "14b0d92b-3a51-4738-f98d-5de5dffc4991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(train_data.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     id                                              tweet subtask_a\n",
            "0  1172  @USER Οι μουσουλμάνες που τις βιάζουν έτσι κ α...       OFF\n",
            "1  4078  Η Κάτια προσπαθεί να πείσει οτι δεν είναι ελέφ...       NOT\n",
            "2   135  Καλά γιατί λες ότι, είσαι νέος αφού γεννήθηκες...       NOT\n",
            "3  9056          Με Φατσεα ξεκινησαμε...... #Kokkinopotami       NOT\n",
            "4  5344                  #gntmgr Κάτια πόσο γλυκιά, εμετός       NOT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9m0rLVi5asp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "03e83092-9117-4e26-d4f3-1f7f777b8739"
      },
      "source": [
        "print(test_data_labels.head())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      0    1\n",
            "0  2707  NOT\n",
            "1  2251  NOT\n",
            "2  9814  NOT\n",
            "3  8949  NOT\n",
            "4  6913  NOT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNiAyB8wNynp",
        "colab_type": "code",
        "outputId": "5f8a90cb-18db-45cd-e6cd-27c14c297966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_data.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8743, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPrf57eYrAHU",
        "colab_type": "code",
        "outputId": "dedc7799-28a5-4f58-9c5e-eb6d33596fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(test_data.shape)\n",
        "print(test_data.head())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1544, 2)\n",
            "     id                                              tweet\n",
            "0  2707  @USER Θέλω να των δω από εδώ και εμπρός αν δεν...\n",
            "1  2251  #survivorgr Α Και 60 φορές και με διαφορετικού...\n",
            "2  9814  Και μου έλεγε η γυναίκα μου το πρωί πάρε την τ...\n",
            "3  8949                   κατω τα χερια απο τον #κυρανακης\n",
            "4  6913  @USER μην μας το παιζεις πονοψυχη,κρυφορατσιστ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmKLSZ1ZCmlr",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAYPsuH7Cwqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lQnedCazcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgRrcqFTa8JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG69bfBpCmCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train,df_dev=train_test_split(train_data,test_size=0.1,shuffle=True,random_state=42,stratify=train_data['subtask_a'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCwwGLvpC0MQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=(df_train['subtask_a']==\"OFF\").astype(int)\n",
        "y_dev=(df_dev['subtask_a']==\"OFF\").astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gU8XkeM5j85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test=(test_data_labels[1]==\"OFF\").astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Y2drVDDMOJ",
        "colab_type": "code",
        "outputId": "40233d94-46f7-4159-f952-bdf5146e60ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_data.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8743, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeSkMR5rDFGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words =5000\n",
        "max_len = 40\n",
        "tok = Tokenizer(max_words)\n",
        "tok.fit_on_texts(train_data['tweet'].astype(str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECe8Cw7PDWzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_train = tok.texts_to_sequences(df_train['tweet'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97BorB4kDgyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_dev = tok.texts_to_sequences(df_dev['tweet'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IoEQG-rDlwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_gK1z32GN_O",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0R0kDUkGMNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index=tok.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqNlCi5nn71M",
        "colab_type": "code",
        "outputId": "ad7fd267-32c1-4245-ae5b-17c29925c0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.41.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvMYqskUGeo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znStVwbnGmG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from sklearn.utils import class_weight\n",
        "class Attention(Layer):\n",
        "    def __init__(self,step_dim=20,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, Dropout\n",
        "\n",
        "#max_len=\n",
        "\n",
        "def BidLstm(maxlen, max_features, embed_size):\n",
        "    inp1 = Input(shape=(maxlen, ))\n",
        "    #inp2=Input(shape=(1,))\n",
        "    x1=Embedding(max_words+1,embed_size)(inp1)\n",
        "    #x1 = Embedding(max_words + 1,embed_size,weights=[embedding_matrix_1],\n",
        "    #              trainable=True)(inp1)\n",
        "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x1)\n",
        "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x2)\n",
        "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x3)   \n",
        "    #x1 = Attention(maxlen)(x1)\n",
        "    # x2 = Attention(maxlen)(x2)\n",
        "    # x3 = Attention(maxlen)(x3)\n",
        "    # x=  Concatenate()([x1,x2,x3])\n",
        "    # x1=MultiHead(layer=keras.layers.Bidirectional(keras.layers.CuDNNLSTM(units=100), name='LSTM'),\n",
        "    # layer_num=5,\n",
        "    # reg_index=[1, 4],\n",
        "    # reg_slice=(slice(None, None), slice(32, 48)),\n",
        "    # reg_factor=0.1,\n",
        "    # name='Multi-Head-Attention')(x1)\n",
        "    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x1)\n",
        "    # x1 = SeqSelfAttention(kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "    #                    bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "    #                    attention_regularizer_weight=1e-4,\n",
        "    #                    name='Attention')(x1)   \n",
        "    x2=  GlobalMaxPooling1D()(x1)\n",
        "    x3= GlobalAveragePooling1D()(x1)\n",
        "    x=  Concatenate()([x2,x3])\n",
        "    x = Dropout(0.1)(x)\n",
        "    #x = Attention(maxlen)(x)\n",
        "    # layer = Dense(600,name='FC1')(x)\n",
        "    # layer = Dense(300,activation='relu')(layer)\n",
        "    layer = Dense(128,activation='relu')(x)\n",
        " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
        "    #layer = Activation('relu')(layer)\n",
        "    #layer = Dropout(0.4)(layer)\n",
        "    layer = Dense(64,name='FC2')(layer)\n",
        "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.4)(layer)\n",
        "   # layer=  Concatenate()([layer,inp2])\n",
        "    # layer=Dense(256,activation='relu')(layer)\n",
        "    # layer=Dense(128,activation='relu')(layer)\n",
        "    layer = Dense(1,name='out_layer',activation='sigmoid')(layer)\n",
        "\n",
        "    model = Model(inputs=[inp1],outputs=layer)\n",
        "\n",
        "    return model\n",
        "model_bi=BidLstm(max_len,max_features=max_words,embed_size=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNt2iKeEGrgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6553afd1-7830-4382-bb17-acabae533a16"
      },
      "source": [
        "model_bi.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc',km.f1_score()])"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_60:0' shape=() dtype=int32> tp\n",
            "tracking <tf.Variable 'Variable_61:0' shape=() dtype=int32> fp\n",
            "tracking <tf.Variable 'Variable_62:0' shape=() dtype=int32> tp\n",
            "tracking <tf.Variable 'Variable_63:0' shape=() dtype=int32> fn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjQYKV-kGwPO",
        "colab_type": "code",
        "outputId": "3d1b757b-5353-4590-d182-4099f4bceb28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "model_bi.summary()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_16 (InputLayer)           (None, 40)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 40, 300)      1500300     input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_9 (Bidirectional) (None, 40, 512)      1142784     embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_16 (Global (None, 512)          0           bidirectional_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_16 (Gl (None, 512)          0           bidirectional_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 1024)         0           global_max_pooling1d_16[0][0]    \n",
            "                                                                 global_average_pooling1d_16[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 1024)         0           concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 128)          131200      dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "FC2 (Dense)                     (None, 64)           8256        dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 64)           0           FC2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 64)           0           activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "out_layer (Dense)               (None, 1)            65          dropout_32[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 2,782,605\n",
            "Trainable params: 2,782,605\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZPeqtXbCrRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=(train_data['subtask_a']==\"OFF\").astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhqzfTntGyHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y),y)\n",
        "class_weights=dict(enumerate(class_weights))\n",
        "class_weights[1]=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1a3cDLzG09-",
        "colab_type": "code",
        "outputId": "f9bc326c-41a2-4ef1-948a-c51c77390cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(class_weights)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.6986575035959726, 1: 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVpE0RLEG23A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_filepath='/content/drive/My Drive/offenseval/'+'checkpoints/lstm_model_2020a_greek.h5'\n",
        "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
        "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=2,restore_best_weights=True)\n",
        "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC8_JSdcG8hP",
        "colab_type": "code",
        "outputId": "bde28f31-f706-4831-a8a8-1c978d72d681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "model_bi.fit([sequences_matrix_train],y_train,validation_data=([sequences_matrix_dev],y_dev),epochs=2,batch_size=32,callbacks=[es,cp_check_point])"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7868 samples, validate on 875 samples\n",
            "Epoch 1/2\n",
            "7868/7868 [==============================] - 4s 566us/step - loss: 0.5312 - acc: 0.7632 - f1_score: 0.1648 - val_loss: 0.4497 - val_acc: 0.8171 - val_f1_score: 0.5916\n",
            "Epoch 2/2\n",
            "7868/7868 [==============================] - 3s 405us/step - loss: 0.3572 - acc: 0.8585 - f1_score: 0.7201 - val_loss: 0.4691 - val_acc: 0.8137 - val_f1_score: 0.5879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7ff09f3fbb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbDV_eeDG_Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_dl=model_bi.predict(sequences_matrix_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfsVlw4_HppI",
        "colab_type": "code",
        "outputId": "d89d28fc-66c5-4e36-9631-55325cbd14c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "# y_pred = model_bi.predict(X_dev, batch_size=30, verbose=1)\n",
        "\n",
        "print(classification_report(y_dev, y_preds_dl.round(),digits=4))"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8284    0.9329    0.8775       626\n",
            "           1     0.7529    0.5141    0.6110       249\n",
            "\n",
            "    accuracy                         0.8137       875\n",
            "   macro avg     0.7907    0.7235    0.7443       875\n",
            "weighted avg     0.8069    0.8137    0.8017       875\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS6onBSrw_JK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_test = tok.texts_to_sequences(test_data['tweet'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA49Bc2uxCzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds=model_bi.predict([sequences_matrix_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJoDnWTk_zph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ac3fa3fb-ceb2-4596-de25-30dcb6f920ea"
      },
      "source": [
        "print(classification_report(y_test, preds.round(),digits=4))\n"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9660    0.9616    0.9638      1302\n",
            "           1     0.7984    0.8182    0.8082       242\n",
            "\n",
            "    accuracy                         0.9391      1544\n",
            "   macro avg     0.8822    0.8899    0.8860      1544\n",
            "weighted avg     0.9398    0.9391    0.9394      1544\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om4Ta-gvxE9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=[]\n",
        "for i in range(len(preds)):\n",
        "  if preds[i].round()==1:\n",
        "    a.append(\"OFF\")\n",
        "  else:\n",
        "    a.append(\"NOT\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtrWrNXuxHL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame({\"ids\":test_data['id'],\"preds\":a},index=None)\n",
        "df.to_csv(root_path+'/non_english_data/Greek/submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}