{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offens_2020_danish.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asking28/offenseval2020/blob/master/offens_2020_danish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0BLcJep5y5Z",
        "colab_type": "code",
        "outputId": "d62a34d2-6d14-47b8-c1d9-a4ac745ea974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAypiho1V6pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a2a37ee-928c-4766-b01d-5b9fd0c93ef3"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhHvBtlLvj1a",
        "colab_type": "code",
        "outputId": "2baf06c2-946e-4f1c-8124-b041796ef0b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install focal-loss\n",
        "!pip install keras-tcn==2.8.3\n",
        "!pip install keras-multi-head\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting focal-loss\n",
            "  Downloading https://files.pythonhosted.org/packages/66/ed/17450291228192ad8595de4514c8ec28a587697b03c707d12d4af5b7f331/focal_loss-0.0.2-py3-none-any.whl\n",
            "Installing collected packages: focal-loss\n",
            "Successfully installed focal-loss-0.0.2\n",
            "Collecting keras-tcn==2.8.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/c4/438c86b27ab11a79cc659d8d6878682c4eb80caa0c0b3d620740cef762f5/keras_tcn-2.8.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn==2.8.3) (1.18.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn==2.8.3) (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn==2.8.3) (1.4.1)\n",
            "Installing collected packages: keras-tcn\n",
            "Successfully installed keras-tcn-2.8.3\n",
            "Collecting keras-multi-head\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (1.18.2)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (2.3.1)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.1.0)\n",
            "Building wheels for collected packages: keras-multi-head, keras-self-attention\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=bcf41dd16a4d1e6443b14ef4c5ebcc03a0452ba04ab3ed4e5813344ec64c061a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=b891999bd64cb1c72dfd01897d603ee56f5e0886ec67731ea9ca533298551a5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-multi-head keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-multi-head\n",
            "Successfully installed keras-multi-head-0.22.0 keras-self-attention-0.41.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t3mWlV357Bw",
        "colab_type": "code",
        "outputId": "fd8c1941-e19c-404e-c6de-d673eee65000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import io\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Conv1D, Conv2D\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
        "from keras.layers import average\n",
        "import tensorflow_hub as hub\n",
        "from keras.layers import Average\n",
        "from keras.layers import Concatenate\n",
        "nltk.download('punkt')\n",
        "from numpy import random\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "from keras.layers import SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "from focal_loss import BinaryFocalLoss\n",
        "from keras_multi_head import MultiHeadAttention\n",
        "from keras_multi_head import MultiHead\n",
        "#plt.switch_backend('agg')\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ZRPEX16GqP",
        "colab_type": "code",
        "outputId": "2a54a84f-8d38-4e3a-edfa-99f73e92db0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install keras_metrics\n",
        "import keras_metrics as km"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.18.2)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU4vkM4u6Wln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "root_path='/content/drive/My Drive/offenseval/2020'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrWhJIr16hwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=pd.read_csv(root_path+'/non_english_data/Danish/offenseval-da-training-v1.tsv',delimiter='\\t',quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f0kkyko-uv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data=pd.read_csv(root_path+'/non_english_data/Danish/offenseval-da-test-v1-nolabels.tsv',delimiter='\\t',quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmjDV4MfikiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_labels=pd.read_csv(root_path+'/non_english_data/Danish/danish-goldlabels.csv',header=None,quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQPlZ_Y2-33f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open(root_path+'/non_english_data/Danish/offenseval-da-test-v1-nolabels.tsv','r') as f:\n",
        "#   lines=f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc636TWCABvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFlhPGsD_jSP",
        "colab_type": "code",
        "outputId": "db7c9a10-fd4e-4c76-8d6d-a2e98c68a89b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(test_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(329, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kJeDyJS_rnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ7OozNa7iOJ",
        "colab_type": "code",
        "outputId": "b8ebad48-a12b-41d3-9528-183292518966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "print(train_data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     id                                              tweet subtask_a\n",
            "0  3131  Jeg tror det vil være dejlig køligt, men jeg v...       NOT\n",
            "1   711  Så kommer de nok til at investere i en ny cyke...       NOT\n",
            "2  2500  Nu er det jo også de Ikea-aber der har lavet s...       OFF\n",
            "3  2678  128 Varme emails, er vi enige om at det er sex...       NOT\n",
            "4   784  Desværre tyder det på, at amerikanerne er helt...       NOT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gILGZUQ099gv",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPoF5hph99Pt",
        "colab_type": "code",
        "outputId": "8dac4278-586b-49d5-97bd-6772930820c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgmvM4P3-CA6",
        "colab_type": "code",
        "outputId": "8ecc77b7-16b8-41f0-95a6-ae35d7fc0c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "def remove_pattern(input_txt, pattern,with_space=False):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    if with_space==False:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    else:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, ' ', input_txt)\n",
        "    return input_txt \n",
        "!pip install emoji\n",
        "import emoji\n",
        "import pickle\n",
        "import re\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/contractions.pkl','rb')as f:\n",
        "  contractions=pickle.load(f)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "contractions=Counter(contractions)\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/acronyms.pkl','rb')as f:\n",
        "  acronyms=pickle.load(f)\n",
        "acronyms=Counter(acronyms)\n",
        "def acronym(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if acronyms[word]!=0:\n",
        "        w_l.append(acronyms[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "# with open('/content/drive/My Drive/Sentimix/hinglish_to_english.pickle','rb')as f:\n",
        "#   hing_to_eng=pickle.load(f)\n",
        "# hing_to_eng=Counter(hing_to_eng)\n",
        "def hindi_se_english(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    w_l=[]\n",
        "    sent=str(df[column][i])\n",
        "    for word in sent.split():\n",
        "      if hing_to_eng[word]!=0:\n",
        "        w_l.append(hing_to_eng[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "# with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
        "#     cuss_dict=pickle.load(handle)\n",
        "# cuss_dict=Counter(cuss_dict)\n",
        "def replace_cuss(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if cuss_dict[word]!=0:\n",
        "        w_l.append('abuse')\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_contraction(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if contractions[word]!=0:\n",
        "        w_l.append(contractions[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_pattern_rep(input_txt, pattern,rep_pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "      input_txt = re.sub(i, rep_pattern, input_txt)\n",
        "\n",
        "    return input_txt \n",
        "def cleaning(data_f,cleaning_col,new_col):\n",
        "  data_f.reset_index(drop=True,inplace=True)\n",
        "  for i in tqdm(range(data_f.shape[0])):\n",
        "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
        "  #data_f[cleaning_col]=replace_cuss(data_f,cleaning_col)\n",
        "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[cleaning_col],\"_\",with_space=True)\n",
        "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
        "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
        "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@[\\w]*\",\"<USR>\")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"http\\S+\",\"<URL>\")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[cleaning_col], \"[0-9]+\",\"<NUM>\")\n",
        "  #data_f[new_col]=hindi_se_english(data_f,cleaning_col)\n",
        "  data_f[new_col]=remove_contraction(data_f,new_col)\n",
        "  data_f[new_col]=acronym(data_f,new_col)\n",
        "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
        "  #data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "  return data_f\n",
        "import numpy as np\n",
        "#a=cleaning(data,'text','clean_col')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 20.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=0d71875da7275c1a9753b6a1e5d19ad52dfea85056145f330f43deb3ebf43205\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7KFLqDW-Qv5",
        "colab_type": "code",
        "outputId": "238ee1f6-7a78-4ac8-ef9f-0b1a2b48851e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data=cleaning(train_data,'tweet','clean_col')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2961/2961 [00:01<00:00, 2146.92it/s]\n",
            "100%|██████████| 2961/2961 [00:00<00:00, 30811.55it/s]\n",
            "100%|██████████| 2961/2961 [00:00<00:00, 26768.63it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W206RnsxAUvB",
        "colab_type": "code",
        "outputId": "fc637532-a7f8-4bee-fd04-5ca537c7693b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "test_data=cleaning(test_data,'tweet','clean_col')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/329 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "100%|██████████| 329/329 [00:00<00:00, 2604.38it/s]\n",
            "100%|██████████| 329/329 [00:00<00:00, 24928.66it/s]\n",
            "100%|██████████| 329/329 [00:00<00:00, 22636.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMf96xQZAc8z",
        "colab_type": "code",
        "outputId": "9b5054af-7189-4294-aeff-3668cb727c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import re, random\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "to_sample = False # if you're impatient switch this flag\n",
        "\n",
        "def spacy_tokenize(text):\n",
        "    return [token.text for token in nlp.tokenizer(text)]\n",
        "    \n",
        "def dameraulevenshtein(seq1, seq2):\n",
        "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
        "    This method has not been modified from the original.\n",
        "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
        "    This distance is the number of additions, deletions, substitutions,\n",
        "    and transpositions needed to transform the first sequence into the\n",
        "    second. Although generally used with strings, any sequences of\n",
        "    comparable objects will work.\n",
        "    Transpositions are exchanges of *consecutive* characters; all other\n",
        "    operations are self-explanatory.\n",
        "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
        "    lengths of the two sequences.\n",
        "    >>> dameraulevenshtein('ba', 'abc')\n",
        "    2\n",
        "    >>> dameraulevenshtein('fee', 'deed')\n",
        "    2\n",
        "    It works with arbitrary sequences too:\n",
        "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
        "    2\n",
        "    \"\"\"\n",
        "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
        "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
        "    # However, only the current and two previous rows are needed at once,\n",
        "    # so we only store those.\n",
        "    oneago = None\n",
        "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
        "    for x in range(len(seq1)):\n",
        "        # Python lists wrap around for negative indices, so put the\n",
        "        # leftmost column at the *end* of the list. This matches with\n",
        "        # the zero-indexed strings and saves extra calculation.\n",
        "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
        "        for y in range(len(seq2)):\n",
        "            delcost = oneago[y] + 1\n",
        "            addcost = thisrow[y - 1] + 1\n",
        "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
        "            thisrow[y] = min(delcost, addcost, subcost)\n",
        "            # This block deals with transpositions\n",
        "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
        "                    and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
        "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
        "    return thisrow[len(seq2) - 1]\n",
        "\n",
        "\n",
        "class SymSpell:\n",
        "    def __init__(self, max_edit_distance=3, verbose=0):\n",
        "        self.max_edit_distance = max_edit_distance\n",
        "        self.verbose = verbose\n",
        "        # 0: top suggestion\n",
        "        # 1: all suggestions of smallest edit distance\n",
        "        # 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
        "\n",
        "        self.dictionary = {}\n",
        "        self.longest_word_length = 0\n",
        "\n",
        "    def get_deletes_list(self, w):\n",
        "        \"\"\"given a word, derive strings with up to max_edit_distance characters\n",
        "           deleted\"\"\"\n",
        "\n",
        "        deletes = []\n",
        "        queue = [w]\n",
        "        for d in range(self.max_edit_distance):\n",
        "            temp_queue = []\n",
        "            for word in queue:\n",
        "                if len(word) > 1:\n",
        "                    for c in range(len(word)):  # character index\n",
        "                        word_minus_c = word[:c] + word[c + 1:]\n",
        "                        if word_minus_c not in deletes:\n",
        "                            deletes.append(word_minus_c)\n",
        "                        if word_minus_c not in temp_queue:\n",
        "                            temp_queue.append(word_minus_c)\n",
        "            queue = temp_queue\n",
        "\n",
        "        return deletes\n",
        "\n",
        "    def create_dictionary_entry(self, w):\n",
        "        '''add word and its derived deletions to dictionary'''\n",
        "        # check if word is already in dictionary\n",
        "        # dictionary entries are in the form: (list of suggested corrections,\n",
        "        # frequency of word in corpus)\n",
        "        new_real_word_added = False\n",
        "        if w in self.dictionary:\n",
        "            # increment count of word in corpus\n",
        "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
        "        else:\n",
        "            self.dictionary[w] = ([], 1)\n",
        "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
        "\n",
        "        if self.dictionary[w][1] == 1:\n",
        "            # first appearance of word in corpus\n",
        "            # n.b. word may already be in dictionary as a derived word\n",
        "            # (deleting character from a real word)\n",
        "            # but counter of frequency of word in corpus is not incremented\n",
        "            # in those cases)\n",
        "            new_real_word_added = True\n",
        "            deletes = self.get_deletes_list(w)\n",
        "            for item in deletes:\n",
        "                if item in self.dictionary:\n",
        "                    # add (correct) word to delete's suggested correction list\n",
        "                    self.dictionary[item][0].append(w)\n",
        "                else:\n",
        "                    # note frequency of word in corpus is not incremented\n",
        "                    self.dictionary[item] = ([w], 0)\n",
        "\n",
        "        return new_real_word_added\n",
        "\n",
        "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        for line in arr:\n",
        "            # separate by words by non-alphabetical characters\n",
        "            words = re.findall(token_pattern, line.lower())\n",
        "            for word in words:\n",
        "                total_word_count += 1\n",
        "                if self.create_dictionary_entry(word):\n",
        "                    unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def create_dictionary(self, fname):\n",
        "        total_word_count = 0\n",
        "        unique_word_count = 0\n",
        "\n",
        "        with open(fname) as file:\n",
        "            for line in file:\n",
        "                # separate by words by non-alphabetical characters\n",
        "                words = re.findall('[a-z]+', line.lower())\n",
        "                for word in words:\n",
        "                    total_word_count += 1\n",
        "                    if self.create_dictionary_entry(word):\n",
        "                        unique_word_count += 1\n",
        "\n",
        "        print(\"total words processed: %i\" % total_word_count)\n",
        "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
        "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
        "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
        "        return self.dictionary\n",
        "\n",
        "    def get_suggestions(self, string, silent=False):\n",
        "        \"\"\"return list of suggested corrections for potentially incorrectly\n",
        "           spelled word\"\"\"\n",
        "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
        "            if not silent:\n",
        "                print(\"no items in dictionary within maximum edit distance\")\n",
        "            return []\n",
        "\n",
        "        suggest_dict = {}\n",
        "        min_suggest_len = float('inf')\n",
        "\n",
        "        queue = [string]\n",
        "        q_dictionary = {}  # items other than string that we've checked\n",
        "\n",
        "        while len(queue) > 0:\n",
        "            q_item = queue[0]  # pop\n",
        "            queue = queue[1:]\n",
        "\n",
        "            # early exit\n",
        "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
        "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
        "                break\n",
        "\n",
        "            # process queue item\n",
        "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
        "                if self.dictionary[q_item][1] > 0:\n",
        "                    # word is in dictionary, and is a word from the corpus, and\n",
        "                    # not already in suggestion list so add to suggestion\n",
        "                    # dictionary, indexed by the word with value (frequency in\n",
        "                    # corpus, edit distance)\n",
        "                    # note q_items that are not the input string are shorter\n",
        "                    # than input string since only deletes are added (unless\n",
        "                    # manual dictionary corrections are added)\n",
        "                    assert len(string) >= len(q_item)\n",
        "                    suggest_dict[q_item] = (self.dictionary[q_item][1],\n",
        "                                            len(string) - len(q_item))\n",
        "                    # early exit\n",
        "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
        "                        break\n",
        "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
        "                        min_suggest_len = len(string) - len(q_item)\n",
        "\n",
        "                # the suggested corrections for q_item as stored in\n",
        "                # dictionary (whether or not q_item itself is a valid word\n",
        "                # or merely a delete) can be valid corrections\n",
        "                for sc_item in self.dictionary[q_item][0]:\n",
        "                    if sc_item not in suggest_dict:\n",
        "\n",
        "                        # compute edit distance\n",
        "                        # suggested items should always be longer\n",
        "                        # (unless manual corrections are added)\n",
        "                        assert len(sc_item) > len(q_item)\n",
        "\n",
        "                        # q_items that are not input should be shorter\n",
        "                        # than original string\n",
        "                        # (unless manual corrections added)\n",
        "                        assert len(q_item) <= len(string)\n",
        "\n",
        "                        if len(q_item) == len(string):\n",
        "                            assert q_item == string\n",
        "                            item_dist = len(sc_item) - len(q_item)\n",
        "\n",
        "                        # item in suggestions list should not be the same as\n",
        "                        # the string itself\n",
        "                        assert sc_item != string\n",
        "\n",
        "                        # calculate edit distance using, for example,\n",
        "                        # Damerau-Levenshtein distance\n",
        "                        item_dist = dameraulevenshtein(sc_item, string)\n",
        "\n",
        "                        # do not add words with greater edit distance if\n",
        "                        # verbose setting not on\n",
        "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
        "                            pass\n",
        "                        elif item_dist <= self.max_edit_distance:\n",
        "                            assert sc_item in self.dictionary  # should already be in dictionary if in suggestion list\n",
        "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
        "                            if item_dist < min_suggest_len:\n",
        "                                min_suggest_len = item_dist\n",
        "\n",
        "                        # depending on order words are processed, some words\n",
        "                        # with different edit distances may be entered into\n",
        "                        # suggestions; trim suggestion dictionary if verbose\n",
        "                        # setting not on\n",
        "                        if self.verbose < 2:\n",
        "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
        "\n",
        "            # now generate deletes (e.g. a substring of string or of a delete)\n",
        "            # from the queue item\n",
        "            # as additional items to check -- add to end of queue\n",
        "            assert len(string) >= len(q_item)\n",
        "\n",
        "            # do not add words with greater edit distance if verbose setting\n",
        "            # is not on\n",
        "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
        "                pass\n",
        "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
        "                for c in range(len(q_item)):  # character index\n",
        "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
        "                    if word_minus_c not in q_dictionary:\n",
        "                        queue.append(word_minus_c)\n",
        "                        q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
        "\n",
        "        # queue is now empty: convert suggestions in dictionary to\n",
        "        # list for output\n",
        "        if not silent and self.verbose != 0:\n",
        "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
        "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
        "\n",
        "        # output option 1\n",
        "        # sort results by ascending order of edit distance and descending\n",
        "        # order of frequency\n",
        "        #     and return list of suggested word corrections only:\n",
        "        # return sorted(suggest_dict, key = lambda x:\n",
        "        #               (suggest_dict[x][1], -suggest_dict[x][0]))\n",
        "\n",
        "        # output option 2\n",
        "        # return list of suggestions with (correction,\n",
        "        #                                  (frequency in corpus, edit distance)):\n",
        "        as_list = suggest_dict.items()\n",
        "        # outlist = sorted(as_list, key=lambda (term, (freq, dist)): (dist, -freq))\n",
        "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
        "\n",
        "        if self.verbose == 0:\n",
        "            return outlist[0]\n",
        "        else:\n",
        "            return outlist\n",
        "\n",
        "        '''\n",
        "        Option 1:\n",
        "        ['file', 'five', 'fire', 'fine', ...]\n",
        "        Option 2:\n",
        "        [('file', (5, 0)),\n",
        "         ('five', (67, 1)),\n",
        "         ('fire', (54, 1)),\n",
        "         ('fine', (17, 1))...]  \n",
        "        '''\n",
        "\n",
        "    def best_word(self, s, silent=False):\n",
        "        try:\n",
        "            return self.get_suggestions(s, silent)[0]\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def spell_corrector(word_list, words_d) -> str:\n",
        "    result_list = []\n",
        "    for word in word_list:\n",
        "        if word not in words_d:\n",
        "            suggestion = ss.best_word(word, silent=True)\n",
        "            if suggestion is not None:\n",
        "                result_list.append(suggestion)\n",
        "        else:\n",
        "            result_list.append(word)\n",
        "            \n",
        "    return \" \".join(result_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # build symspell tree \n",
        "    ss = SymSpell(max_edit_distance=2)\n",
        "    \n",
        "    # fetch list of bad words\n",
        "    with open('/content/drive/My Drive/offenseval/2020/cleaner_data/bad-words.csv') as bf:\n",
        "        bad_words = bf.readlines()\n",
        "    bad_words = [word.strip() for word in bad_words]    \n",
        "    \n",
        "    # fetch english words dictionary\n",
        "    with open('/content/drive/My Drive/offenseval/2020/cleaner_data/english_words_479k.txt') as f:\n",
        "        words = f.readlines()\n",
        "    eng_words = [word.strip() for word in words]\n",
        "    \n",
        "    # Print some examples\n",
        "    print(eng_words[:5])\n",
        "    print(bad_words[:5])\n",
        "\n",
        "    print('Total english words: {}'.format(len(eng_words)))\n",
        "    print('Total bad words: {}'.format(len(bad_words)))\n",
        "    \n",
        "    print('create symspell dict...')\n",
        "    \n",
        "    if to_sample:\n",
        "        # sampling from list for kernel runtime\n",
        "        sample_idxs = random.sample(range(len(eng_words)), 100)\n",
        "        eng_words = [eng_words[i] for i in sorted(sample_idxs)] + \\\n",
        "            'to infinity and beyond'.split() # make sure our sample misspell is in there\n",
        "    \n",
        "    all_words_list = list(set(bad_words + eng_words))\n",
        "    silence = ss.create_dictionary_from_arr(all_words_list, token_pattern=r'.+')\n",
        "    \n",
        "    # create a dictionary of rightly spelled words for lookup\n",
        "    words_dict = {k: 0 for k in all_words_list}\n",
        "    \n",
        "    sample_text = 'to infifity and byond'\n",
        "    \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2', '1080', '&c', '10-point', '10th']\n",
            "['jigaboo', 'mound of venus', 'asslover', 's&m', 'queaf']\n",
            "Total english words: 466544\n",
            "Total bad words: 1617\n",
            "create symspell dict...\n",
            "total words processed: 467594\n",
            "total unique words in corpus: 467394\n",
            "total items in dictionary (corpus words and deletions): 20250415\n",
            "  edit distance for deletions: 2\n",
            "  length of longest word in corpus: 45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyiZpLVAAtfQ",
        "colab_type": "code",
        "outputId": "5f341bfa-c62a-4fe0-a9a3-e18864cbb2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_data['tokens'] = test_data['clean_col'].apply(spacy_tokenize)\n",
        "    \n",
        "print('run spell checker...')\n",
        "print()\n",
        "#print('original text: ' + sample_text)\n",
        "print()\n",
        "test_data['clean'] = test_data.apply(lambda row:spell_corrector(row['tokens'],words_dict),axis=1)\n",
        "#print('corrected text: ' + correct_text)\n",
        "\n",
        "print('Done.')  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run spell checker...\n",
            "\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MvVfKANAyiM",
        "colab_type": "code",
        "outputId": "158a9ba2-15f6-47f2-9432-6f8eed3a7ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_data['tokens'] = train_data['clean_col'].apply(spacy_tokenize)\n",
        "    \n",
        "print('run spell checker...')\n",
        "print()\n",
        "#print('original text: ' + sample_text)\n",
        "print()\n",
        "train_data['clean'] = train_data.apply(lambda row:spell_corrector(row['tokens'],words_dict),axis=1)\n",
        "#print('corrected text: ' + correct_text)\n",
        "\n",
        "print('Done.')  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run spell checker...\n",
            "\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmKLSZ1ZCmlr",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAYPsuH7Cwqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1lQnedCazcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgRrcqFTa8JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold=StratifiedKFold(n_splits=10,shuffle=True,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG69bfBpCmCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train,df_dev=train_test_split(train_data,test_size=0.1,random_state=2020)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCwwGLvpC0MQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=(df_train['subtask_a']==\"OFF\").astype(int)\n",
        "y_dev=(df_dev['subtask_a']==\"OFF\").astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdqoLiIqj2cH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f78c8f01-57c5-4911-98d9-57a071aef2b0"
      },
      "source": [
        "test_data_labels.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1382</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1384</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>547</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1269</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1695</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1\n",
              "0  1382  NOT\n",
              "1  1384  NOT\n",
              "2   547  NOT\n",
              "3  1269  NOT\n",
              "4  1695  OFF"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2F2ttOpjxkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test=(test_data_labels[1]==\"OFF\").astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Y2drVDDMOJ",
        "colab_type": "code",
        "outputId": "4e27d680-fd10-407c-d6dd-da21cab8370a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(train_data.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2961, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeSkMR5rDFGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words =3000\n",
        "max_len = 45\n",
        "tok = Tokenizer(max_words)\n",
        "tok.fit_on_texts(train_data['clean'].astype(str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECe8Cw7PDWzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_train = tok.texts_to_sequences(df_train['clean'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97BorB4kDgyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_dev = tok.texts_to_sequences(df_dev['clean'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xni5tuPKkC6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_test = tok.texts_to_sequences(test_data['clean'].astype(str))\n",
        "# vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IoEQG-rDlwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_gK1z32GN_O",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0R0kDUkGMNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index=tok.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqNlCi5nn71M",
        "colab_type": "code",
        "outputId": "3cdade01-a2b1-4bf5-cbb4-7a0e4c54034b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.41.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.18.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvMYqskUGeo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znStVwbnGmG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from sklearn.utils import class_weight\n",
        "class Attention(Layer):\n",
        "    def __init__(self,step_dim=20,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, Dropout\n",
        "\n",
        "#max_len=\n",
        "\n",
        "def BidLstm(maxlen, max_features, embed_size):\n",
        "    inp1 = Input(shape=(maxlen, ))\n",
        "    #inp2=Input(shape=(1,))\n",
        "    x1=Embedding(max_words+1,embed_size)(inp1)\n",
        "    #x1 = Embedding(max_words + 1,embed_size,weights=[embedding_matrix_1],\n",
        "    #              trainable=True)(inp1)\n",
        "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x1)\n",
        "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x2)\n",
        "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x3)   \n",
        "    #x1 = Attention(maxlen)(x1)\n",
        "    # x2 = Attention(maxlen)(x2)\n",
        "    # x3 = Attention(maxlen)(x3)\n",
        "    # x=  Concatenate()([x1,x2,x3])\n",
        "    # x1=MultiHead(layer=keras.layers.Bidirectional(keras.layers.CuDNNLSTM(units=200), name='LSTM'),\n",
        "    # layer_num=5,\n",
        "    # reg_index=[1, 4],\n",
        "    # reg_slice=(slice(None, None), slice(32, 48)),\n",
        "    # reg_factor=0.1,\n",
        "    # name='Multi-Head-Attention')(x1)\n",
        "    x1 = CuDNNLSTM(256, return_sequences=True)(x1)   \n",
        "    x1 = SeqSelfAttention(kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                    bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                    attention_regularizer_weight=1e-4,\n",
        "                    name='Attention')(x1) \n",
        "    \n",
        "    x2=  GlobalMaxPooling1D()(x1)\n",
        "    x3= GlobalAveragePooling1D()(x1)\n",
        "    x=  Concatenate()([x2,x3])\n",
        "    x = Dropout(0.1)(x)\n",
        "    #x = Attention(maxlen)(x)\n",
        "    # layer = Dense(600,name='FC1')(x)\n",
        "    # layer = Dense(300,activation='relu')(layer)\n",
        "    layer = Dense(128,activation='relu')(x)\n",
        " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
        "    #layer = Activation('relu')(layer)\n",
        "    #layer = Dropout(0.4)(layer)\n",
        "    layer = Dense(64,name='FC2')(layer)\n",
        "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.4)(layer)\n",
        "   # layer=  Concatenate()([layer,inp2])\n",
        "    # layer=Dense(256,activation='relu')(layer)\n",
        "    # layer=Dense(128,activation='relu')(layer)\n",
        "    layer = Dense(1,name='out_layer',activation='sigmoid')(layer)\n",
        "\n",
        "    model = Model(inputs=[inp1],outputs=layer)\n",
        "\n",
        "    return model\n",
        "model_bi=BidLstm(max_len,max_features=max_words,embed_size=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNt2iKeEGrgl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0545cad4-9220-4d03-a640-3cc6b0f8a5ea"
      },
      "source": [
        "model_bi.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc',km.f1_score()])"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tracking <tf.Variable 'Variable_36:0' shape=() dtype=int32> tp\n",
            "tracking <tf.Variable 'Variable_37:0' shape=() dtype=int32> fp\n",
            "tracking <tf.Variable 'Variable_38:0' shape=() dtype=int32> tp\n",
            "tracking <tf.Variable 'Variable_39:0' shape=() dtype=int32> fn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjQYKV-kGwPO",
        "colab_type": "code",
        "outputId": "273e20be-b59f-4a2d-bb4d-c3dea6cbafd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "model_bi.summary()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 45)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 45, 300)      900300      input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_9 (CuDNNLSTM)        (None, 45, 256)      571392      embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Attention (SeqSelfAttention)    (None, 45, 256)      16449       cu_dnnlstm_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_9 (GlobalM (None, 256)          0           Attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_9 (Glo (None, 256)          0           Attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_9[0][0]     \n",
            "                                                                 global_average_pooling1d_9[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 512)          0           concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 128)          65664       dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "FC2 (Dense)                     (None, 64)           8256        dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 64)           0           FC2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 64)           0           activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "out_layer (Dense)               (None, 1)            65          dropout_18[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,562,126\n",
            "Trainable params: 1,562,126\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhqzfTntGyHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
        "class_weights=dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1a3cDLzG09-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "766eb7f3-7ae1-4b5e-a945-86c31f5014f0"
      },
      "source": [
        "print(class_weights)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.5733964700817907, 1: 3.906158357771261}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1xKGqI9mETl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights[1]=3.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVpE0RLEG23A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_filepath='/content/drive/My Drive/offenseval/'+'checkpoints/lstm_model_2020a_danish.h5'\n",
        "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
        "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=2,restore_best_weights=True)\n",
        "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC8_JSdcG8hP",
        "colab_type": "code",
        "outputId": "465fc2b4-1b1e-41f0-adf5-1a834a2b4878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "model_bi.fit([sequences_matrix_train],y_train,validation_data=([sequences_matrix_dev],y_dev),epochs=2,batch_size=32,class_weight=class_weights,callbacks=[es,cp_check_point])"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2664 samples, validate on 297 samples\n",
            "Epoch 1/2\n",
            "2664/2664 [==============================] - 3s 1ms/step - loss: 0.6692 - acc: 0.8074 - f1_score: 0.1107 - val_loss: 0.6536 - val_acc: 0.7374 - val_f1_score: 0.2747\n",
            "Epoch 2/2\n",
            "2664/2664 [==============================] - 1s 540us/step - loss: 0.5549 - acc: 0.7920 - f1_score: 0.3653 - val_loss: 0.4682 - val_acc: 0.8620 - val_f1_score: 0.5548\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd503700f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbDV_eeDG_Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_dl=model_bi.predict(sequences_matrix_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfsVlw4_HppI",
        "colab_type": "code",
        "outputId": "e6f3e52b-c29e-4ae3-fb84-09997d371846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model_bi.predict(sequences_matrix_dev, batch_size=30, verbose=1)\n",
        "\n",
        "print(classification_report(y_dev, y_pred.round(),digits=4))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "297/297 [==============================] - 0s 128us/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9176    0.9213    0.9194       254\n",
            "           1     0.5238    0.5116    0.5176        43\n",
            "\n",
            "    accuracy                         0.8620       297\n",
            "   macro avg     0.7207    0.7164    0.7185       297\n",
            "weighted avg     0.8606    0.8620    0.8613       297\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2825HggFjUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds=model_bi.predict([sequences_matrix_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hmiCe_BlHMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "2601d784-1479-4d99-e9a3-4f043823488d"
      },
      "source": [
        "print(classification_report(y_test, preds.round(),digits=4))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9234    0.8785    0.9004       288\n",
            "           1     0.3636    0.4878    0.4167        41\n",
            "\n",
            "    accuracy                         0.8298       329\n",
            "   macro avg     0.6435    0.6831    0.6585       329\n",
            "weighted avg     0.8536    0.8298    0.8401       329\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5UI8EUEFm6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=[]\n",
        "for i in range(len(preds)):\n",
        "  if preds[i].round()==1:\n",
        "    a.append(\"OFF\")\n",
        "  else:\n",
        "    a.append(\"NOT\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC-sQVa9GRsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame({\"ids\":test_data['id'],\"preds\":a},index=None)\n",
        "df.to_csv(root_path+'/non_english_data/Danish/submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0_LAoXWHglb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}