{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offens_2020_eng_task1_twitter_emb_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GDvTes-Kn62L"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi29jeRMxM_s",
        "colab_type": "code",
        "outputId": "d4d21468-af25-45f7-b69a-3b13597ba534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MESIg6zsxc70",
        "outputId": "ef1c64d6-e770-4a65-fde1-0c7bfa956dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VTkjWBstxcf9",
        "outputId": "741d26f0-6427-49cf-e60b-dfa9e9580b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import io\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Conv1D, Conv2D\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
        "from keras.layers import average\n",
        "import tensorflow_hub as hub\n",
        "from keras.layers import Average\n",
        "from keras.layers import Concatenate\n",
        "nltk.download('punkt')\n",
        "from numpy import random\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "from keras.layers import SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "#plt.switch_backend('agg')\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzK80S4F7DdF",
        "colab_type": "code",
        "outputId": "95d5f704-939d-4b5c-a4a4-bb3543369d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "!pip install keras_metrics\n",
        "!pip install emoji\n",
        "import keras_metrics as km"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.17.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=5182208690ec316ac24db1160a862a0d947422c55ab0dfdbf37ef24ad7469a74\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRcYiWz-7JZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path='/content/drive/My Drive/offenseval/2020'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs_8bCmn7N9f",
        "colab_type": "code",
        "outputId": "f8f3698e-fb72-4c69-9dd8-82d58ac5b8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "os.listdir(root_path+\"/chunky\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data_0',\n",
              " 'data_1',\n",
              " 'data_2',\n",
              " 'data_3',\n",
              " 'data_4',\n",
              " 'data_5',\n",
              " 'data_6',\n",
              " 'data_7',\n",
              " 'data_8',\n",
              " 'data_9',\n",
              " 'data_10',\n",
              " 'data_11',\n",
              " 'data_12',\n",
              " 'data_13',\n",
              " 'data_14',\n",
              " 'data_15',\n",
              " 'data_16',\n",
              " 'data_17',\n",
              " 'data_18',\n",
              " 'data_19',\n",
              " 'data_20',\n",
              " 'data_21',\n",
              " 'data_23',\n",
              " 'data_24',\n",
              " 'data_25',\n",
              " 'data_26',\n",
              " 'data_27',\n",
              " 'data_28',\n",
              " 'data_29',\n",
              " 'data_30',\n",
              " 'data_31',\n",
              " 'data_32',\n",
              " 'data_33',\n",
              " 'data_34',\n",
              " 'data_35',\n",
              " 'data_36',\n",
              " 'data_37',\n",
              " 'data_38',\n",
              " 'data_39',\n",
              " 'data_40',\n",
              " 'data_41',\n",
              " 'data_42',\n",
              " 'data_43',\n",
              " 'data_50',\n",
              " 'data_44',\n",
              " 'data_45',\n",
              " 'data_51',\n",
              " 'data_52',\n",
              " 'data_46',\n",
              " 'data_53',\n",
              " 'data_47',\n",
              " 'data_54',\n",
              " 'data_48',\n",
              " 'data_55',\n",
              " 'data_49',\n",
              " 'data_56',\n",
              " 'data_57',\n",
              " 'data_100',\n",
              " 'data_101',\n",
              " 'data_102',\n",
              " 'data_103',\n",
              " 'data_104',\n",
              " 'data_105',\n",
              " 'data_107',\n",
              " 'data_108',\n",
              " 'data_109',\n",
              " 'data_110',\n",
              " 'data_111',\n",
              " 'data_112',\n",
              " 'data_113',\n",
              " 'data_114',\n",
              " 'data_115',\n",
              " 'data_116',\n",
              " 'data_117',\n",
              " 'data_118',\n",
              " 'data_119',\n",
              " 'data_59',\n",
              " 'data_60',\n",
              " 'data_61',\n",
              " 'data_62',\n",
              " 'data_63',\n",
              " 'data_64',\n",
              " 'data_65',\n",
              " 'data_66',\n",
              " 'data_67',\n",
              " 'data_68',\n",
              " 'data_69',\n",
              " 'data_70',\n",
              " 'data_71',\n",
              " 'data_72',\n",
              " 'data_73',\n",
              " 'data_74',\n",
              " 'data_75',\n",
              " 'data_76',\n",
              " 'data_77',\n",
              " 'data_78',\n",
              " 'data_79',\n",
              " 'data_80',\n",
              " 'data_82',\n",
              " 'data_83',\n",
              " 'data_84',\n",
              " 'data_85',\n",
              " 'data_86',\n",
              " 'data_87',\n",
              " 'data_88',\n",
              " 'data_89',\n",
              " 'data_90',\n",
              " 'data_91',\n",
              " 'data_92',\n",
              " 'data_93',\n",
              " 'data_94',\n",
              " 'data_96',\n",
              " 'data_97',\n",
              " 'data_98',\n",
              " 'data_99']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHxggQHy7a70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from zipfile import ZipFile\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-oZOquV85D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file_name = root_path+\"/task_a_distant.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHgBv7oj9d6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with ZipFile(file_name,'r') as zip:\n",
        "#   zip.printdir()\n",
        "#   zip.extractall(root_path,pwd=b'sem2020-t12')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TBcQvff9sqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data=pd.read_csv('/content/drive/My Drive/offenseval/2020/task_a_distant.tsv',delimiter='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn-RHzseEUE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(train_data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4_swfRnnrD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_data['text'].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EQNlWkrnNbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_chunky=np.array_split(train_data,120)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1p4nbWoLanc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gc\n",
        "# del(train_data)\n",
        "# gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfuFZci1u9PW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# len(df_chunky)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apbAMbN3IlDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w36B594KIomg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels=(train_data['average']>0.5).astype(int)\n",
        "# print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXPSjW_tFp4q",
        "colab_type": "code",
        "outputId": "cca163aa-8af6-4a4b-a376-d0dfff81fa76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "list_df=os.listdir(root_path+\"/chunky\")\n",
        "print(list_df)\n",
        "print(len(list_df))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['data_0', 'data_1', 'data_2', 'data_3', 'data_4', 'data_5', 'data_6', 'data_7', 'data_8', 'data_9', 'data_10', 'data_11', 'data_12', 'data_13', 'data_14', 'data_15', 'data_16', 'data_17', 'data_18', 'data_19', 'data_20', 'data_21', 'data_23', 'data_24', 'data_25', 'data_26', 'data_27', 'data_28', 'data_29', 'data_30', 'data_31', 'data_32', 'data_33', 'data_34', 'data_35', 'data_36', 'data_37', 'data_38', 'data_39', 'data_40', 'data_41', 'data_42', 'data_43', 'data_50', 'data_44', 'data_45', 'data_51', 'data_52', 'data_46', 'data_53', 'data_47', 'data_54', 'data_48', 'data_55', 'data_49', 'data_56', 'data_57', 'data_100', 'data_101', 'data_102', 'data_103', 'data_104', 'data_105', 'data_107', 'data_108', 'data_109', 'data_110', 'data_111', 'data_112', 'data_113', 'data_114', 'data_115', 'data_116', 'data_117', 'data_118', 'data_119', 'data_59', 'data_60', 'data_61', 'data_62', 'data_63', 'data_64', 'data_65', 'data_66', 'data_67', 'data_68', 'data_69', 'data_70', 'data_71', 'data_72', 'data_73', 'data_74', 'data_75', 'data_76', 'data_77', 'data_78', 'data_79', 'data_80', 'data_82', 'data_83', 'data_84', 'data_85', 'data_86', 'data_87', 'data_88', 'data_89', 'data_90', 'data_91', 'data_92', 'data_93', 'data_94', 'data_96', 'data_97', 'data_98', 'data_99']\n",
            "115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k89d6hLKFqr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_dummy=pd.read_csv(root_path+\"/chunky/data_0\")\n",
        "for file_name in list_df:\n",
        "  if file_name != 'data_0':\n",
        "    df_dummy=df_dummy.append(pd.read_csv(root_path+\"/chunky/\"+file_name),ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4m-I9yDmuJN",
        "colab_type": "text"
      },
      "source": [
        "## Twitter embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TGj5a5ylxOB",
        "colab_type": "code",
        "outputId": "7b6c23ec-741f-467a-dee8-c2dd66626a1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Sentimix/word2vec_twitter_tokens.bin', binary=True,unicode_errors='ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD-JkrV5m0Vu",
        "colab_type": "code",
        "outputId": "eca5e821-bc62-4a2e-e4ca-8237a50c743d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model['fuck'].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDvTes-Kn62L",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob8t7HZzvFwe",
        "colab_type": "code",
        "outputId": "7630029f-f729-4af3-82cf-86f440e1d0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1NH5q2QnTq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_pattern(input_txt, pattern,with_space=False):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    if with_space==False:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    else:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, ' ', input_txt)\n",
        "    return input_txt \n",
        "\n",
        "import emoji\n",
        "import pickle\n",
        "import re\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/contractions.pkl','rb')as f:\n",
        "  contractions=pickle.load(f)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "contractions=Counter(contractions)\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/acronyms.pkl','rb')as f:\n",
        "  acronyms=pickle.load(f)\n",
        "acronyms=Counter(acronyms)\n",
        "def acronym(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if acronyms[word]!=0:\n",
        "        w_l.append(acronyms[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "with open('/content/drive/My Drive/Sentimix/hinglish_to_english.pickle','rb')as f:\n",
        "  hing_to_eng=pickle.load(f)\n",
        "hing_to_eng=Counter(hing_to_eng)\n",
        "def hindi_se_english(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    w_l=[]\n",
        "    sent=str(df[column][i])\n",
        "    for word in sent.split():\n",
        "      if hing_to_eng[word]!=0:\n",
        "        w_l.append(hing_to_eng[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
        "    cuss_dict=pickle.load(handle)\n",
        "cuss_dict=Counter(cuss_dict)\n",
        "def replace_cuss(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if cuss_dict[word]!=0:\n",
        "        w_l.append('abuse')\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_contraction(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if contractions[word]!=0:\n",
        "        w_l.append(contractions[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_pattern_rep(input_txt, pattern,rep_pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "      input_txt = re.sub(i, rep_pattern, input_txt)\n",
        "\n",
        "    return input_txt \n",
        "def cleaning(data_f,cleaning_col,new_col):\n",
        "  data_f.reset_index(drop=True,inplace=True)\n",
        "  for i in tqdm(range(data_f.shape[0])):\n",
        "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
        "  # #data_f[cleaning_col]=replace_cuss(data_f,cleaning_col)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[cleaning_col],\"_\",with_space=True)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
        "  # data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@[\\w]*\",\"<USR>\")\n",
        "  # data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"http\\S+\",\"<URL>\")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[cleaning_col], \"[0-9]+\",\"<NUM>\")\n",
        "  #data_f[new_col]=hindi_se_english(data_f,cleaning_col)\n",
        "  data_f[new_col]=remove_contraction(data_f,new_col)\n",
        "  data_f[new_col]=acronym(data_f,new_col)\n",
        "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
        "  #data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "  #data_f.drop(['text'],inplace=True)\n",
        "  return data_f\n",
        "import numpy as np\n",
        "#a=cleaning(df_chunky[0],'text','clean_col')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRqIrCrjzMqo",
        "colab_type": "code",
        "outputId": "b56cae96-4bc0-4fe5-c59d-601dba70e166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_chunky[0].columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'text', 'average', 'std'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgYhtXx0zcHg",
        "colab_type": "code",
        "outputId": "f1adc96d-7239-468b-9bc8-93296084c018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "str(2)+\"_ho\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2_ho'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWuKxMcn0Ziy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir -p '/content/drive/My Drive/offenseval/2020/chunky'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FofO6S0P0gxG",
        "colab_type": "code",
        "outputId": "5303a5f5-e8e5-4837-f53e-515f5357965a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.listdir('/content/drive/My Drive/offenseval/2020/chunky')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data_3',\n",
              " 'data_4',\n",
              " 'data_5',\n",
              " 'data_6',\n",
              " 'data_7',\n",
              " 'data_8',\n",
              " 'data_9',\n",
              " 'data_10',\n",
              " 'data_11',\n",
              " 'data_12',\n",
              " 'data_13',\n",
              " 'data_14',\n",
              " 'data_15',\n",
              " 'data_16',\n",
              " 'data_17',\n",
              " 'data_50',\n",
              " 'data_51',\n",
              " 'data_52',\n",
              " 'data_53',\n",
              " 'data_0',\n",
              " 'data_1',\n",
              " 'data_2',\n",
              " 'data_54',\n",
              " 'data_55',\n",
              " 'data_56',\n",
              " 'data_57',\n",
              " 'data_18',\n",
              " 'data_19',\n",
              " 'data_20',\n",
              " 'data_21',\n",
              " 'data_23',\n",
              " 'data_24',\n",
              " 'data_25',\n",
              " 'data_26',\n",
              " 'data_27',\n",
              " 'data_28',\n",
              " 'data_29',\n",
              " 'data_30',\n",
              " 'data_31',\n",
              " 'data_32',\n",
              " 'data_33',\n",
              " 'data_34',\n",
              " 'data_35',\n",
              " 'data_36',\n",
              " 'data_37',\n",
              " 'data_38',\n",
              " 'data_39',\n",
              " 'data_40',\n",
              " 'data_41',\n",
              " 'data_42',\n",
              " 'data_43',\n",
              " 'data_44',\n",
              " 'data_45',\n",
              " 'data_46',\n",
              " 'data_47',\n",
              " 'data_48',\n",
              " 'data_49']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2vCjVMG1YAe",
        "colab_type": "code",
        "outputId": "7d63d76d-446d-403c-f46c-5bf5b60f0d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_chunky[1].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>average</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>90734</th>\n",
              "      <td>1159676112907198464</td>\n",
              "      <td>@USER I would be so scared in that thing in th...</td>\n",
              "      <td>0.770275</td>\n",
              "      <td>0.171736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90735</th>\n",
              "      <td>1159676113096007680</td>\n",
              "      <td>I would love to thank @USER for my friends lol...</td>\n",
              "      <td>0.161673</td>\n",
              "      <td>0.194959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90736</th>\n",
              "      <td>1159642691736625152</td>\n",
              "      <td>After they take away your guns, next it'll be ...</td>\n",
              "      <td>0.351962</td>\n",
              "      <td>0.129857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90737</th>\n",
              "      <td>1159676117826985984</td>\n",
              "      <td>y’all niggas gotta learn how to be independent...</td>\n",
              "      <td>0.703757</td>\n",
              "      <td>0.154905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90738</th>\n",
              "      <td>1159676117932007424</td>\n",
              "      <td>@USER @USER rape, statutory rape, pedophilia, ...</td>\n",
              "      <td>0.520919</td>\n",
              "      <td>0.206871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        id  ...       std\n",
              "90734  1159676112907198464  ...  0.171736\n",
              "90735  1159676113096007680  ...  0.194959\n",
              "90736  1159642691736625152  ...  0.129857\n",
              "90737  1159676117826985984  ...  0.154905\n",
              "90738  1159676117932007424  ...  0.206871\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro9lo90hL8Tp",
        "colab_type": "code",
        "outputId": "3bb88250-2ce1-47cc-ab63-45805979645e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df_chunky[50]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>average</th>\n",
              "      <th>std</th>\n",
              "      <th>clean_col</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1162529925699690497</td>\n",
              "      <td>Kyle Busch with a blown motor wins Stage 2 and...</td>\n",
              "      <td>0.181783</td>\n",
              "      <td>0.186384</td>\n",
              "      <td>kyle busch with a blown motor wins stage &lt;num&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1162523959339831296</td>\n",
              "      <td>@USER @USER Absolutely my pleasure and, as alw...</td>\n",
              "      <td>0.157049</td>\n",
              "      <td>0.172415</td>\n",
              "      <td>@user @user absolutely my pleasure and, as alw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1162529925930221568</td>\n",
              "      <td>Ara’s hobbies are writing lyrics and composing...</td>\n",
              "      <td>0.198885</td>\n",
              "      <td>0.168191</td>\n",
              "      <td>ara’s hobbies are writing lyrics and composing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1162496550695649280</td>\n",
              "      <td>and i officially have my own apartment with my...</td>\n",
              "      <td>0.147311</td>\n",
              "      <td>0.184600</td>\n",
              "      <td>and i officially have my own apartment with my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1162529929764003840</td>\n",
              "      <td>I just got back from Walmart and realized I ha...</td>\n",
              "      <td>0.500671</td>\n",
              "      <td>0.163458</td>\n",
              "      <td>i just got back from walmart and realized i ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75607</th>\n",
              "      <td>1157145533741273088</td>\n",
              "      <td>Tell the old story!  Share it to the next gene...</td>\n",
              "      <td>0.159502</td>\n",
              "      <td>0.169071</td>\n",
              "      <td>tell the old story! share it to the next gener...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75608</th>\n",
              "      <td>1157077318050955265</td>\n",
              "      <td>the perfect night to sink into the floor and die</td>\n",
              "      <td>0.456075</td>\n",
              "      <td>0.220745</td>\n",
              "      <td>the perfect night to sink into the floor and die</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75609</th>\n",
              "      <td>1157145534106234880</td>\n",
              "      <td>Something's got to be done about all those peo...</td>\n",
              "      <td>0.535651</td>\n",
              "      <td>0.068703</td>\n",
              "      <td>something's got to be done about all those peo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75610</th>\n",
              "      <td>1157145534148173826</td>\n",
              "      <td>true love is when they’re the one you want to ...</td>\n",
              "      <td>0.194527</td>\n",
              "      <td>0.159600</td>\n",
              "      <td>true love is when they’re the one you want to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75611</th>\n",
              "      <td>1157145534156566529</td>\n",
              "      <td>I do not want to be around negativity</td>\n",
              "      <td>0.219685</td>\n",
              "      <td>0.160932</td>\n",
              "      <td>i do not want to be around negativity</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75612 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                        id  ...                                          clean_col\n",
              "0      1162529925699690497  ...  kyle busch with a blown motor wins stage <num>...\n",
              "1      1162523959339831296  ...  @user @user absolutely my pleasure and, as alw...\n",
              "2      1162529925930221568  ...  ara’s hobbies are writing lyrics and composing...\n",
              "3      1162496550695649280  ...  and i officially have my own apartment with my...\n",
              "4      1162529929764003840  ...  i just got back from walmart and realized i ha...\n",
              "...                    ...  ...                                                ...\n",
              "75607  1157145533741273088  ...  tell the old story! share it to the next gener...\n",
              "75608  1157077318050955265  ...   the perfect night to sink into the floor and die\n",
              "75609  1157145534106234880  ...  something's got to be done about all those peo...\n",
              "75610  1157145534148173826  ...  true love is when they’re the one you want to ...\n",
              "75611  1157145534156566529  ...              i do not want to be around negativity\n",
              "\n",
              "[75612 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsjv28rqyYHN",
        "colab_type": "code",
        "outputId": "c28aa006-675e-45f0-92b4-c32639a21d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        }
      },
      "source": [
        "for i in range(50,100):\n",
        "  print(i)\n",
        "  a=cleaning(df_chunky[i],'text','clean_col')\n",
        "  df=pd.DataFrame({'text':a['clean_col'],'label':a['average']})\n",
        "  df.to_csv('/content/drive/My Drive/offenseval/2020/chunky/data_'+str(i),index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/75612 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  0%|          | 55/75612 [00:00<02:18, 546.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:07<00:00, 1126.67it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 47608.85it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 48468.47it/s]\n",
            "  0%|          | 110/75612 [00:00<01:09, 1091.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "51\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:08<00:00, 1109.04it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 46775.78it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 47156.30it/s]\n",
            "  0%|          | 99/75612 [00:00<01:16, 987.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "52\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:08<00:00, 1105.88it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 48788.74it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 47435.98it/s]\n",
            "  0%|          | 113/75612 [00:00<01:06, 1128.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "53\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:09<00:00, 1084.48it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 49936.85it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 50100.06it/s]\n",
            "  0%|          | 95/75612 [00:00<01:19, 947.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:10<00:00, 1072.81it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 48223.84it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 48404.15it/s]\n",
            "  0%|          | 105/75612 [00:00<01:11, 1049.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "55\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:09<00:00, 1087.47it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 48572.27it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 47947.41it/s]\n",
            "  0%|          | 102/75612 [00:00<01:14, 1008.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "56\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:08<00:00, 1108.28it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 49356.63it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 47486.13it/s]\n",
            "  0%|          | 99/75612 [00:00<01:16, 982.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "57\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:08<00:00, 1110.39it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 47788.20it/s]\n",
            "100%|██████████| 75612/75612 [00:01<00:00, 48086.71it/s]\n",
            "  0%|          | 113/75612 [00:00<01:06, 1127.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "58\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 75612/75612 [01:08<00:00, 1105.62it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdajs4zjqiF7",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Train test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cUy9gOioPdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGOZ-FwXq1-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train,df_test=train_test_split(df_dummy,test_size=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHeKckLprUHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_data=pd.DataFrame({'text':a['clean_col'],'labels':labels})\n",
        "# df_data.to_csv(root_path+\"/clean_eng_task_1.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKf-drEg14pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words =100000\n",
        "max_len = 25\n",
        "tok = Tokenizer(max_words)\n",
        "tok.fit_on_texts(df_dummy['text'].astype(str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQZs70B_2JrV",
        "colab_type": "code",
        "outputId": "055a4cd3-e29f-4380-e4f3-c6c1fbe19679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tok.word_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user': 1,\n",
              " 'the': 2,\n",
              " 'i': 3,\n",
              " 'to': 4,\n",
              " 'and': 5,\n",
              " 'a': 6,\n",
              " 'of': 7,\n",
              " 'you': 8,\n",
              " 'is': 9,\n",
              " 'in': 10,\n",
              " 'it': 11,\n",
              " 'that': 12,\n",
              " 'for': 13,\n",
              " 'my': 14,\n",
              " 'be': 15,\n",
              " 'this': 16,\n",
              " 'me': 17,\n",
              " 'was': 18,\n",
              " 'on': 19,\n",
              " 'with': 20,\n",
              " 'not': 21,\n",
              " 'so': 22,\n",
              " 'but': 23,\n",
              " 'he': 24,\n",
              " 'just': 25,\n",
              " 'have': 26,\n",
              " 'are': 27,\n",
              " 'like': 28,\n",
              " 'all': 29,\n",
              " 'your': 30,\n",
              " 'if': 31,\n",
              " 'as': 32,\n",
              " 'at': 33,\n",
              " 'what': 34,\n",
              " 'i’m': 35,\n",
              " 'one': 36,\n",
              " 'do': 37,\n",
              " 'they': 38,\n",
              " 'his': 39,\n",
              " 'we': 40,\n",
              " 'up': 41,\n",
              " 'out': 42,\n",
              " 'get': 43,\n",
              " 'when': 44,\n",
              " 'can': 45,\n",
              " 'will': 46,\n",
              " 'had': 47,\n",
              " 'about': 48,\n",
              " 'her': 49,\n",
              " 'love': 50,\n",
              " 'no': 51,\n",
              " 'know': 52,\n",
              " 'how': 53,\n",
              " 'time': 54,\n",
              " 'people': 55,\n",
              " 'it’s': 56,\n",
              " 'from': 57,\n",
              " 'now': 58,\n",
              " 'who': 59,\n",
              " 'good': 60,\n",
              " 'or': 61,\n",
              " 'go': 62,\n",
              " 'really': 63,\n",
              " 'want': 64,\n",
              " \"i'm\": 65,\n",
              " 'don’t': 66,\n",
              " 'think': 67,\n",
              " 'them': 68,\n",
              " 'see': 69,\n",
              " 'an': 70,\n",
              " 'would': 71,\n",
              " 'she': 72,\n",
              " 'why': 73,\n",
              " 'him': 74,\n",
              " 'has': 75,\n",
              " 'only': 76,\n",
              " 'need': 77,\n",
              " 'got': 78,\n",
              " 'back': 79,\n",
              " \"it's\": 80,\n",
              " 'there': 81,\n",
              " 'more': 82,\n",
              " 'too': 83,\n",
              " 'day': 84,\n",
              " 'going': 85,\n",
              " 'some': 86,\n",
              " 'by': 87,\n",
              " 'been': 88,\n",
              " 'shit': 89,\n",
              " 'never': 90,\n",
              " 'still': 91,\n",
              " 'u': 92,\n",
              " 'even': 93,\n",
              " 'best': 94,\n",
              " 'make': 95,\n",
              " 'way': 96,\n",
              " 'because': 97,\n",
              " 'much': 98,\n",
              " 'am': 99,\n",
              " 'right': 100,\n",
              " 'life': 101,\n",
              " 'lol': 102,\n",
              " 'did': 103,\n",
              " \"don't\": 104,\n",
              " 'then': 105,\n",
              " 'say': 106,\n",
              " 'amp': 107,\n",
              " 'should': 108,\n",
              " 'their': 109,\n",
              " 'that’s': 110,\n",
              " 'first': 111,\n",
              " 'off': 112,\n",
              " 'being': 113,\n",
              " 'man': 114,\n",
              " 'same': 115,\n",
              " 'ever': 116,\n",
              " 'us': 117,\n",
              " 'someone': 118,\n",
              " 'well': 119,\n",
              " 'always': 120,\n",
              " 'than': 121,\n",
              " 'fuck': 122,\n",
              " 'take': 123,\n",
              " 'today': 124,\n",
              " 'here': 125,\n",
              " 'can’t': 126,\n",
              " 'im': 127,\n",
              " 'over': 128,\n",
              " 'thing': 129,\n",
              " 'said': 130,\n",
              " 'new': 131,\n",
              " 'our': 132,\n",
              " '😂': 133,\n",
              " 'feel': 134,\n",
              " 'last': 135,\n",
              " '2': 136,\n",
              " 'work': 137,\n",
              " 'could': 138,\n",
              " 'better': 139,\n",
              " 'thank': 140,\n",
              " 'gonna': 141,\n",
              " 'come': 142,\n",
              " 'please': 143,\n",
              " 'most': 144,\n",
              " 'god': 145,\n",
              " 'any': 146,\n",
              " 'where': 147,\n",
              " 'every': 148,\n",
              " 'were': 149,\n",
              " \"that's\": 150,\n",
              " 'other': 151,\n",
              " 'look': 152,\n",
              " 'after': 153,\n",
              " 'game': 154,\n",
              " 'down': 155,\n",
              " '3': 156,\n",
              " 'hope': 157,\n",
              " 'again': 158,\n",
              " 'also': 159,\n",
              " 'these': 160,\n",
              " 'great': 161,\n",
              " 'year': 162,\n",
              " 'something': 163,\n",
              " 'give': 164,\n",
              " 'thought': 165,\n",
              " 'world': 166,\n",
              " 'into': 167,\n",
              " 'let': 168,\n",
              " 'bad': 169,\n",
              " 'happy': 170,\n",
              " 'tell': 171,\n",
              " 'before': 172,\n",
              " 'very': 173,\n",
              " 'ass': 174,\n",
              " 'next': 175,\n",
              " 'hate': 176,\n",
              " 'you’re': 177,\n",
              " 'stop': 178,\n",
              " 'its': 179,\n",
              " 'wanna': 180,\n",
              " '1': 181,\n",
              " 'yes': 182,\n",
              " 'fucking': 183,\n",
              " 'things': 184,\n",
              " 'does': 185,\n",
              " 'getting': 186,\n",
              " 'oh': 187,\n",
              " 'wait': 188,\n",
              " 'person': 189,\n",
              " 'sure': 190,\n",
              " 'keep': 191,\n",
              " 'he’s': 192,\n",
              " 'everyone': 193,\n",
              " 'made': 194,\n",
              " 'i’ve': 195,\n",
              " 'doing': 196,\n",
              " 'put': 197,\n",
              " 'real': 198,\n",
              " 'y’all': 199,\n",
              " 'those': 200,\n",
              " 'actually': 201,\n",
              " 'lot': 202,\n",
              " 'night': 203,\n",
              " \"you're\": 204,\n",
              " 'years': 205,\n",
              " \"can't\": 206,\n",
              " 'girl': 207,\n",
              " 'watch': 208,\n",
              " 'many': 209,\n",
              " 'show': 210,\n",
              " 'long': 211,\n",
              " 'play': 212,\n",
              " 'wish': 213,\n",
              " 'trump': 214,\n",
              " 'money': 215,\n",
              " 'trying': 216,\n",
              " 'already': 217,\n",
              " 'nothing': 218,\n",
              " 'thanks': 219,\n",
              " 'end': 220,\n",
              " 'two': 221,\n",
              " 'friends': 222,\n",
              " 'done': 223,\n",
              " 'tweet': 224,\n",
              " 'hard': 225,\n",
              " 'big': 226,\n",
              " 'everything': 227,\n",
              " 'yeah': 228,\n",
              " 'start': 229,\n",
              " 'i’ll': 230,\n",
              " 'name': 231,\n",
              " 'home': 232,\n",
              " 'twitter': 233,\n",
              " 'find': 234,\n",
              " 'whole': 235,\n",
              " \"he's\": 236,\n",
              " 'mean': 237,\n",
              " 'own': 238,\n",
              " 'myself': 239,\n",
              " 'lmao': 240,\n",
              " 'old': 241,\n",
              " 'sorry': 242,\n",
              " 'anyone': 243,\n",
              " 'literally': 244,\n",
              " '5': 245,\n",
              " 'guy': 246,\n",
              " 'days': 247,\n",
              " 'live': 248,\n",
              " 'week': 249,\n",
              " 'while': 250,\n",
              " 'having': 251,\n",
              " 'anything': 252,\n",
              " 'try': 253,\n",
              " 'another': 254,\n",
              " 'around': 255,\n",
              " '”': 256,\n",
              " 'bitch': 257,\n",
              " 'use': 258,\n",
              " 'believe': 259,\n",
              " 'little': 260,\n",
              " 'didn’t': 261,\n",
              " 'talk': 262,\n",
              " 'help': 263,\n",
              " '4': 264,\n",
              " 'wrong': 265,\n",
              " 'cause': 266,\n",
              " 'call': 267,\n",
              " 'follow': 268,\n",
              " 'part': 269,\n",
              " 'maybe': 270,\n",
              " 'guys': 271,\n",
              " 'away': 272,\n",
              " 'might': 273,\n",
              " 'since': 274,\n",
              " 'through': 275,\n",
              " 'enough': 276,\n",
              " '😭': 277,\n",
              " '10': 278,\n",
              " 'which': 279,\n",
              " 'looking': 280,\n",
              " 'heart': 281,\n",
              " 'season': 282,\n",
              " 'nice': 283,\n",
              " 'both': 284,\n",
              " 'remember': 285,\n",
              " 'makes': 286,\n",
              " 'care': 287,\n",
              " 'dont': 288,\n",
              " 'house': 289,\n",
              " 'team': 290,\n",
              " 'song': 291,\n",
              " 'damn': 292,\n",
              " 'morning': 293,\n",
              " 'miss': 294,\n",
              " 'left': 295,\n",
              " \"i've\": 296,\n",
              " 'school': 297,\n",
              " 'point': 298,\n",
              " 'used': 299,\n",
              " 'read': 300,\n",
              " 'white': 301,\n",
              " 'though': 302,\n",
              " 'ok': 303,\n",
              " 'saying': 304,\n",
              " 'face': 305,\n",
              " 'head': 306,\n",
              " 'mind': 307,\n",
              " 'may': 308,\n",
              " 'sleep': 309,\n",
              " \"i'll\": 310,\n",
              " 'went': 311,\n",
              " 'baby': 312,\n",
              " 'hear': 313,\n",
              " 'sad': 314,\n",
              " 'hell': 315,\n",
              " 'friend': 316,\n",
              " 'coming': 317,\n",
              " 'fun': 318,\n",
              " 'yet': 319,\n",
              " 'probably': 320,\n",
              " 'seen': 321,\n",
              " 'okay': 322,\n",
              " 'looks': 323,\n",
              " 'watching': 324,\n",
              " 'pretty': 325,\n",
              " 'once': 326,\n",
              " 'ain’t': 327,\n",
              " 'must': 328,\n",
              " 'until': 329,\n",
              " 'ready': 330,\n",
              " 'without': 331,\n",
              " 'such': 332,\n",
              " 'gotta': 333,\n",
              " 'ur': 334,\n",
              " 'talking': 335,\n",
              " 'thinking': 336,\n",
              " 'kind': 337,\n",
              " 'tonight': 338,\n",
              " 'stay': 339,\n",
              " 'yourself': 340,\n",
              " 'ask': 341,\n",
              " 'needs': 342,\n",
              " 'saw': 343,\n",
              " 'win': 344,\n",
              " 'told': 345,\n",
              " 'making': 346,\n",
              " 'family': 347,\n",
              " 'leave': 348,\n",
              " 'place': 349,\n",
              " 'true': 350,\n",
              " 'times': 351,\n",
              " 'playing': 352,\n",
              " 'job': 353,\n",
              " 'video': 354,\n",
              " 'change': 355,\n",
              " \"didn't\": 356,\n",
              " 'music': 357,\n",
              " 'guess': 358,\n",
              " 'bro': 359,\n",
              " 'tomorrow': 360,\n",
              " 'else': 361,\n",
              " 'rest': 362,\n",
              " 'top': 363,\n",
              " 'bc': 364,\n",
              " 'full': 365,\n",
              " 'hit': 366,\n",
              " 'wow': 367,\n",
              " 'tired': 368,\n",
              " 'omg': 369,\n",
              " '0': 370,\n",
              " 'reason': 371,\n",
              " 'cute': 372,\n",
              " 'soon': 373,\n",
              " 'few': 374,\n",
              " 'fact': 375,\n",
              " 'eat': 376,\n",
              " 'black': 377,\n",
              " 'free': 378,\n",
              " 'hair': 379,\n",
              " 'doesn’t': 380,\n",
              " 'phone': 381,\n",
              " 'least': 382,\n",
              " 'beautiful': 383,\n",
              " 'worst': 384,\n",
              " 'called': 385,\n",
              " 'nigga': 386,\n",
              " 'hey': 387,\n",
              " 'kids': 388,\n",
              " 'wants': 389,\n",
              " 'gt': 390,\n",
              " 'amazing': 391,\n",
              " 'high': 392,\n",
              " 'feeling': 393,\n",
              " 'funny': 394,\n",
              " 'hours': 395,\n",
              " 'boy': 396,\n",
              " 'buy': 397,\n",
              " 'wanted': 398,\n",
              " 'gone': 399,\n",
              " 'lost': 400,\n",
              " 'tho': 401,\n",
              " 'says': 402,\n",
              " 'heard': 403,\n",
              " 'country': 404,\n",
              " 'post': 405,\n",
              " 'different': 406,\n",
              " 'run': 407,\n",
              " 'w': 408,\n",
              " 'sometimes': 409,\n",
              " 'n': 410,\n",
              " 'came': 411,\n",
              " 'gets': 412,\n",
              " 'half': 413,\n",
              " 'men': 414,\n",
              " 'birthday': 415,\n",
              " 'understand': 416,\n",
              " 'crazy': 417,\n",
              " '6': 418,\n",
              " 'girls': 419,\n",
              " 'cool': 420,\n",
              " 'far': 421,\n",
              " 'idk': 422,\n",
              " 'send': 423,\n",
              " 'movie': 424,\n",
              " 'move': 425,\n",
              " 'word': 426,\n",
              " 'definitely': 427,\n",
              " 'honestly': 428,\n",
              " 'instead': 429,\n",
              " 'mad': 430,\n",
              " 'car': 431,\n",
              " 'listen': 432,\n",
              " 'took': 433,\n",
              " 'party': 434,\n",
              " 'hot': 435,\n",
              " 'president': 436,\n",
              " 'pay': 437,\n",
              " \"doesn't\": 438,\n",
              " 'ya': 439,\n",
              " 'second': 440,\n",
              " 'support': 441,\n",
              " 'mom': 442,\n",
              " 'women': 443,\n",
              " 'bed': 444,\n",
              " 'imagine': 445,\n",
              " 'bit': 446,\n",
              " 'proud': 447,\n",
              " '7': 448,\n",
              " 'cant': 449,\n",
              " 'side': 450,\n",
              " 'almost': 451,\n",
              " 'idea': 452,\n",
              " '😂😂': 453,\n",
              " 'against': 454,\n",
              " 'stuff': 455,\n",
              " 'course': 456,\n",
              " 'problem': 457,\n",
              " 'news': 458,\n",
              " 'taking': 459,\n",
              " 'ago': 460,\n",
              " 'together': 461,\n",
              " 'finally': 462,\n",
              " 'knew': 463,\n",
              " 'single': 464,\n",
              " 'i’d': 465,\n",
              " 'dude': 466,\n",
              " 'stupid': 467,\n",
              " 'waiting': 468,\n",
              " 'able': 469,\n",
              " 'found': 470,\n",
              " 'ones': 471,\n",
              " 'working': 472,\n",
              " 'food': 473,\n",
              " 'comes': 474,\n",
              " 'story': 475,\n",
              " 'glad': 476,\n",
              " 'account': 477,\n",
              " 'started': 478,\n",
              " 'room': 479,\n",
              " 'dead': 480,\n",
              " 'isn’t': 481,\n",
              " 'favorite': 482,\n",
              " 'seeing': 483,\n",
              " 'album': 484,\n",
              " 'sick': 485,\n",
              " 'sex': 486,\n",
              " 'bring': 487,\n",
              " 'games': 488,\n",
              " 'nobody': 489,\n",
              " 'niggas': 490,\n",
              " 'fan': 491,\n",
              " 'past': 492,\n",
              " 'agree': 493,\n",
              " 'anymore': 494,\n",
              " 'words': 495,\n",
              " '8': 496,\n",
              " 'either': 497,\n",
              " 'forget': 498,\n",
              " 'vote': 499,\n",
              " 'check': 500,\n",
              " 'won’t': 501,\n",
              " 'open': 502,\n",
              " 'alone': 503,\n",
              " 'what’s': 504,\n",
              " 'each': 505,\n",
              " 'woman': 506,\n",
              " 'power': 507,\n",
              " 'type': 508,\n",
              " 'truth': 509,\n",
              " 'mine': 510,\n",
              " 'happened': 511,\n",
              " 'there’s': 512,\n",
              " 'cry': 513,\n",
              " 'happen': 514,\n",
              " 'fine': 515,\n",
              " 'matter': 516,\n",
              " 'ppl': 517,\n",
              " 'close': 518,\n",
              " 'question': 519,\n",
              " 'goes': 520,\n",
              " '😂😂😂': 521,\n",
              " 'she’s': 522,\n",
              " 'weird': 523,\n",
              " 'exactly': 524,\n",
              " 'fall': 525,\n",
              " 'eyes': 526,\n",
              " '2019': 527,\n",
              " 'deal': 528,\n",
              " 'turn': 529,\n",
              " 'break': 530,\n",
              " \"i'd\": 531,\n",
              " 'body': 532,\n",
              " 'wonder': 533,\n",
              " 'excited': 534,\n",
              " 'rn': 535,\n",
              " 'number': 536,\n",
              " 'means': 537,\n",
              " 'line': 538,\n",
              " 'fans': 539,\n",
              " 'between': 540,\n",
              " 'worth': 541,\n",
              " 'deserve': 542,\n",
              " 'they’re': 543,\n",
              " 'fight': 544,\n",
              " '100': 545,\n",
              " 'under': 546,\n",
              " 'hand': 547,\n",
              " '20': 548,\n",
              " '30': 549,\n",
              " 'tweets': 550,\n",
              " 'dm': 551,\n",
              " 'month': 552,\n",
              " 'minutes': 553,\n",
              " 'knows': 554,\n",
              " 'summer': 555,\n",
              " 'yo': 556,\n",
              " 'future': 557,\n",
              " \"isn't\": 558,\n",
              " 'lose': 559,\n",
              " '🤣': 560,\n",
              " 'dream': 561,\n",
              " 'die': 562,\n",
              " 'dog': 563,\n",
              " 'moment': 564,\n",
              " 'months': 565,\n",
              " 'hi': 566,\n",
              " 'gave': 567,\n",
              " 'chance': 568,\n",
              " 'fake': 569,\n",
              " 'others': 570,\n",
              " 'b': 571,\n",
              " 'bts': 572,\n",
              " 'less': 573,\n",
              " 'crying': 574,\n",
              " 'pls': 575,\n",
              " 'seems': 576,\n",
              " 'hurt': 577,\n",
              " 'water': 578,\n",
              " 'living': 579,\n",
              " 'self': 580,\n",
              " 'absolutely': 581,\n",
              " 'rt': 582,\n",
              " 'wtf': 583,\n",
              " 'trust': 584,\n",
              " 'weekend': 585,\n",
              " 'late': 586,\n",
              " 'super': 587,\n",
              " 'beat': 588,\n",
              " 'set': 589,\n",
              " '\\U0001f97a': 590,\n",
              " '❤️': 591,\n",
              " 'boys': 592,\n",
              " 'state': 593,\n",
              " '🤔': 594,\n",
              " 'asked': 595,\n",
              " 'tried': 596,\n",
              " 'giving': 597,\n",
              " 'city': 598,\n",
              " '9': 599,\n",
              " 'haven’t': 600,\n",
              " 'x': 601,\n",
              " 'media': 602,\n",
              " 'perfect': 603,\n",
              " 'wasn’t': 604,\n",
              " 'using': 605,\n",
              " 'haha': 606,\n",
              " 'enjoy': 607,\n",
              " 'class': 608,\n",
              " 'loved': 609,\n",
              " \"there's\": 610,\n",
              " 'dad': 611,\n",
              " 'kid': 612,\n",
              " 'drop': 613,\n",
              " '🙄': 614,\n",
              " 'list': 615,\n",
              " 'during': 616,\n",
              " 'front': 617,\n",
              " 'dumb': 618,\n",
              " 'learn': 619,\n",
              " 'd': 620,\n",
              " 'kinda': 621,\n",
              " 'book': 622,\n",
              " 'listening': 623,\n",
              " 'bitches': 624,\n",
              " 's': 625,\n",
              " 'o': 626,\n",
              " 'forgot': 627,\n",
              " 'three': 628,\n",
              " 'red': 629,\n",
              " 'played': 630,\n",
              " \"won't\": 631,\n",
              " 'sounds': 632,\n",
              " 'early': 633,\n",
              " 'stand': 634,\n",
              " 'dick': 635,\n",
              " 'yesterday': 636,\n",
              " 'hold': 637,\n",
              " 'business': 638,\n",
              " 'energy': 639,\n",
              " 'reply': 640,\n",
              " 'sense': 641,\n",
              " 'picture': 642,\n",
              " \"what's\": 643,\n",
              " 'straight': 644,\n",
              " 'lie': 645,\n",
              " 'hour': 646,\n",
              " 'answer': 647,\n",
              " 'lt': 648,\n",
              " 'become': 649,\n",
              " \"they're\": 650,\n",
              " 'hands': 651,\n",
              " 'kill': 652,\n",
              " 'watched': 653,\n",
              " 'group': 654,\n",
              " 'thats': 655,\n",
              " 'cut': 656,\n",
              " 'asking': 657,\n",
              " 'likes': 658,\n",
              " 'racist': 659,\n",
              " 'supposed': 660,\n",
              " 'pain': 661,\n",
              " 'death': 662,\n",
              " 'weeks': 663,\n",
              " 'swear': 664,\n",
              " 'fire': 665,\n",
              " 'whatever': 666,\n",
              " 'brother': 667,\n",
              " 'yall': 668,\n",
              " 'lord': 669,\n",
              " 'telling': 670,\n",
              " 'america': 671,\n",
              " 'act': 672,\n",
              " 'meet': 673,\n",
              " 'cannot': 674,\n",
              " 'respect': 675,\n",
              " 'case': 676,\n",
              " 'felt': 677,\n",
              " 'scared': 678,\n",
              " 'luck': 679,\n",
              " 't': 680,\n",
              " 'forever': 681,\n",
              " 'easy': 682,\n",
              " 'joke': 683,\n",
              " 'relationship': 684,\n",
              " 'behind': 685,\n",
              " 'we’re': 686,\n",
              " 'pick': 687,\n",
              " 'son': 688,\n",
              " 'bout': 689,\n",
              " 'child': 690,\n",
              " 'young': 691,\n",
              " 'couple': 692,\n",
              " 'jesus': 693,\n",
              " 'later': 694,\n",
              " 'level': 695,\n",
              " 'let’s': 696,\n",
              " 'r': 697,\n",
              " 'entire': 698,\n",
              " 'somebody': 699,\n",
              " 'smh': 700,\n",
              " 'middle': 701,\n",
              " '😩': 702,\n",
              " 'voice': 703,\n",
              " '12': 704,\n",
              " 'date': 705,\n",
              " 'feels': 706,\n",
              " \"she's\": 707,\n",
              " 'tbh': 708,\n",
              " 'light': 709,\n",
              " 'walk': 710,\n",
              " 'running': 711,\n",
              " \"we're\": 712,\n",
              " 'drink': 713,\n",
              " \"let's\": 714,\n",
              " 'human': 715,\n",
              " 'shot': 716,\n",
              " 'sound': 717,\n",
              " 'league': 718,\n",
              " 'peace': 719,\n",
              " 'needed': 720,\n",
              " 'biggest': 721,\n",
              " 'cuz': 722,\n",
              " 'speak': 723,\n",
              " 'save': 724,\n",
              " 'small': 725,\n",
              " 'songs': 726,\n",
              " 'rather': 727,\n",
              " 'wake': 728,\n",
              " 'starting': 729,\n",
              " 'bet': 730,\n",
              " 'parents': 731,\n",
              " 'eating': 732,\n",
              " 'tf': 733,\n",
              " 'awesome': 734,\n",
              " 'welcome': 735,\n",
              " 'meant': 736,\n",
              " 'king': 737,\n",
              " 'important': 738,\n",
              " 'episode': 739,\n",
              " 'calling': 740,\n",
              " 'reading': 741,\n",
              " 'shows': 742,\n",
              " 'forward': 743,\n",
              " 'ball': 744,\n",
              " 'takes': 745,\n",
              " 'pass': 746,\n",
              " 'broke': 747,\n",
              " 'nah': 748,\n",
              " 'sit': 749,\n",
              " 'soul': 750,\n",
              " 'order': 751,\n",
              " '15': 752,\n",
              " 'mouth': 753,\n",
              " 'club': 754,\n",
              " 'till': 755,\n",
              " 'shut': 756,\n",
              " 'ugly': 757,\n",
              " 'wear': 758,\n",
              " 'worse': 759,\n",
              " 'truly': 760,\n",
              " 'player': 761,\n",
              " 'attention': 762,\n",
              " '😔': 763,\n",
              " 'missed': 764,\n",
              " 'outside': 765,\n",
              " 'wife': 766,\n",
              " 'lil': 767,\n",
              " '11': 768,\n",
              " 'himself': 769,\n",
              " 'tryna': 770,\n",
              " 'thread': 771,\n",
              " 'throw': 772,\n",
              " 'appreciate': 773,\n",
              " 'everybody': 774,\n",
              " 'seriously': 775,\n",
              " 'strong': 776,\n",
              " 'e': 777,\n",
              " 'fucked': 778,\n",
              " 'age': 779,\n",
              " 'fast': 780,\n",
              " 'history': 781,\n",
              " 'law': 782,\n",
              " 'wouldn’t': 783,\n",
              " 'sign': 784,\n",
              " 'retweet': 785,\n",
              " 'poor': 786,\n",
              " 'followers': 787,\n",
              " '“': 788,\n",
              " 'share': 789,\n",
              " 'mood': 790,\n",
              " 'office': 791,\n",
              " 'sir': 792,\n",
              " 'football': 793,\n",
              " 'won': 794,\n",
              " 'none': 795,\n",
              " 'tv': 796,\n",
              " \"y'all\": 797,\n",
              " 'app': 798,\n",
              " 'social': 799,\n",
              " '\\U0001f974': 800,\n",
              " 'sweet': 801,\n",
              " '50': 802,\n",
              " 'cold': 803,\n",
              " 'dark': 804,\n",
              " '“i': 805,\n",
              " 'inside': 806,\n",
              " 'add': 807,\n",
              " 'text': 808,\n",
              " 'looked': 809,\n",
              " 'following': 810,\n",
              " 'short': 811,\n",
              " 'realize': 812,\n",
              " 'happens': 813,\n",
              " 'series': 814,\n",
              " 'pic': 815,\n",
              " 'trash': 816,\n",
              " 'gay': 817,\n",
              " 'drive': 818,\n",
              " 'public': 819,\n",
              " 'piece': 820,\n",
              " 'especially': 821,\n",
              " 'low': 822,\n",
              " 'bought': 823,\n",
              " 'pray': 824,\n",
              " 'art': 825,\n",
              " 'smile': 826,\n",
              " 'control': 827,\n",
              " 'cat': 828,\n",
              " '😊': 829,\n",
              " 'safe': 830,\n",
              " 'tl': 831,\n",
              " 'possible': 832,\n",
              " 'anyway': 833,\n",
              " 'works': 834,\n",
              " \"wasn't\": 835,\n",
              " 'plan': 836,\n",
              " 'brain': 837,\n",
              " 'gun': 838,\n",
              " 'united': 839,\n",
              " 'goal': 840,\n",
              " 'feelings': 841,\n",
              " 'paid': 842,\n",
              " 'fair': 843,\n",
              " 'american': 844,\n",
              " 'everyday': 845,\n",
              " 'children': 846,\n",
              " 'sent': 847,\n",
              " 'character': 848,\n",
              " 'expect': 849,\n",
              " 'write': 850,\n",
              " 'deep': 851,\n",
              " 'woke': 852,\n",
              " 'amount': 853,\n",
              " 'spend': 854,\n",
              " 'wearing': 855,\n",
              " 'sister': 856,\n",
              " 'stan': 857,\n",
              " 'putting': 858,\n",
              " 'couldn’t': 859,\n",
              " 'af': 860,\n",
              " 'government': 861,\n",
              " 'catch': 862,\n",
              " 'laugh': 863,\n",
              " 'special': 864,\n",
              " 'blue': 865,\n",
              " 'ima': 866,\n",
              " 'match': 867,\n",
              " 'college': 868,\n",
              " 'players': 869,\n",
              " 'drunk': 870,\n",
              " 'comment': 871,\n",
              " 'totally': 872,\n",
              " 'serious': 873,\n",
              " 'met': 874,\n",
              " 'experience': 875,\n",
              " 'link': 876,\n",
              " 'bag': 877,\n",
              " 'pussy': 878,\n",
              " 'f': 879,\n",
              " 'coffee': 880,\n",
              " 'version': 881,\n",
              " 'annoying': 882,\n",
              " 'halloween': 883,\n",
              " 'block': 884,\n",
              " 'mother': 885,\n",
              " 'lady': 886,\n",
              " 'choice': 887,\n",
              " 'loves': 888,\n",
              " \"haven't\": 889,\n",
              " 'clear': 890,\n",
              " 'pictures': 891,\n",
              " 'bruh': 892,\n",
              " 'seem': 893,\n",
              " 'killed': 894,\n",
              " 'born': 895,\n",
              " '😍': 896,\n",
              " 'quite': 897,\n",
              " 'police': 898,\n",
              " 'dear': 899,\n",
              " 'thinks': 900,\n",
              " 'air': 901,\n",
              " 'pics': 902,\n",
              " 'lots': 903,\n",
              " 'war': 904,\n",
              " 'choose': 905,\n",
              " 'boyfriend': 906,\n",
              " 'lives': 907,\n",
              " 'finish': 908,\n",
              " 'grow': 909,\n",
              " 'august': 910,\n",
              " 'given': 911,\n",
              " 'opinion': 912,\n",
              " 'worry': 913,\n",
              " 'fat': 914,\n",
              " 'earth': 915,\n",
              " 'taken': 916,\n",
              " 'c': 917,\n",
              " 'bless': 918,\n",
              " 'eye': 919,\n",
              " 'except': 920,\n",
              " 'videos': 921,\n",
              " 'you’ll': 922,\n",
              " 'longer': 923,\n",
              " 'update': 924,\n",
              " 'issue': 925,\n",
              " 'knowing': 926,\n",
              " 'thoughts': 927,\n",
              " 'hello': 928,\n",
              " 'rich': 929,\n",
              " 'suck': 930,\n",
              " 'unless': 931,\n",
              " 'door': 932,\n",
              " 'rain': 933,\n",
              " 'na': 934,\n",
              " 'you’ve': 935,\n",
              " 'due': 936,\n",
              " 'em': 937,\n",
              " 'fear': 938,\n",
              " \"wouldn't\": 939,\n",
              " 'join': 940,\n",
              " '2020': 941,\n",
              " 'completely': 942,\n",
              " 'final': 943,\n",
              " 'sitting': 944,\n",
              " 'sis': 945,\n",
              " '👀': 946,\n",
              " 'step': 947,\n",
              " 'changed': 948,\n",
              " 'friday': 949,\n",
              " 'road': 950,\n",
              " 'huge': 951,\n",
              " 'movies': 952,\n",
              " 'happening': 953,\n",
              " 'mate': 954,\n",
              " 'along': 955,\n",
              " 'mf': 956,\n",
              " \"ain't\": 957,\n",
              " \"you'll\": 958,\n",
              " 'ice': 959,\n",
              " 'star': 960,\n",
              " 'didnt': 961,\n",
              " '😭😭': 962,\n",
              " 'yours': 963,\n",
              " 'afraid': 964,\n",
              " 'missing': 965,\n",
              " 'health': 966,\n",
              " 'allowed': 967,\n",
              " 'fit': 968,\n",
              " 'minute': 969,\n",
              " 'sucks': 970,\n",
              " 'bill': 971,\n",
              " 'surprised': 972,\n",
              " 'sunday': 973,\n",
              " 'greatest': 974,\n",
              " 'interesting': 975,\n",
              " 'turned': 976,\n",
              " 'lying': 977,\n",
              " 'aren’t': 978,\n",
              " 'actual': 979,\n",
              " 'gym': 980,\n",
              " 'bunch': 981,\n",
              " 'ah': 982,\n",
              " 'imma': 983,\n",
              " 'facts': 984,\n",
              " 'mental': 985,\n",
              " 'm': 986,\n",
              " 'father': 987,\n",
              " 'stream': 988,\n",
              " 'yea': 989,\n",
              " 'john': 990,\n",
              " 'who’s': 991,\n",
              " 'alive': 992,\n",
              " 'idiot': 993,\n",
              " 'k': 994,\n",
              " 'hopefully': 995,\n",
              " 'happiness': 996,\n",
              " 'gives': 997,\n",
              " 'period': 998,\n",
              " '24': 999,\n",
              " \"you've\": 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Srg4Qd13Tx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_train = tok.texts_to_sequences(df_train['text'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbA-ZTG3cjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_dev = tok.texts_to_sequences(df_test['text'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLBowSka3j2f",
        "colab_type": "code",
        "outputId": "32688b86-5ae2-4c0a-89c4-82090a47fc74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "sequences_matrix_train[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,    1,    1,  197,    3, 1379,    4, 1943,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5sIsAoeHcLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=(df_train['label']>=0.5).astype(int)\n",
        "y_test=(df_test['label']>0.5).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACTqMkJk7z4d",
        "colab_type": "code",
        "outputId": "f7ec9d19-7cb2-4141-c57b-649fb7c4474d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X61XUiBv74br",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFV-OY1q72ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/drive/My Drive/IR_project/glove.6B', 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hXg2G_K8EZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index=tok.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFIF22aV8GVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix_1 = np.zeros([max_words + 1, 400])\n",
        "for word, i in tok.word_index.items():\n",
        "    if word in model:\n",
        "      if i>max_words:\n",
        "        break\n",
        "      embedding_matrix_1[i] = model[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjcjoPEw8I9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8aCgdtglcg5",
        "colab_type": "code",
        "outputId": "0e41e842-65cc-4c4d-c36d-018d3bea0e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/44/3e/eb1a7c7545eede073ceda2f5d78442b6cad33b5b750d7f0742866907c34b/keras-self-attention-0.42.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.17.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.42.0-cp36-none-any.whl size=17296 sha256=508ac992502617cdee7eb3298476326a2a7ebfc9e6c52478bb997a676a122acc\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/05/a0/99c0cf60d383f0494e10eca2b238ea98faca9a1fe03cac2894\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhq0rZOk8N4M",
        "colab_type": "code",
        "outputId": "b3797025-d40e-4772-b094-9c1a878afc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from sklearn.utils import class_weight\n",
        "class Attention(Layer):\n",
        "    def __init__(self,step_dim=20,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, Dropout\n",
        "\n",
        "#max_len=\n",
        "\n",
        "def BidLstm(maxlen, max_features, embed_size):\n",
        "    inp1 = Input(shape=(maxlen, ))\n",
        "    #inp2=Input(shape=(1,))\n",
        "    #x=Embedding(len(word_index)+1,embed_size)(inp1)\n",
        "    x1 = Embedding(max_words + 1,embed_size,weights=[embedding_matrix_1],\n",
        "                  trainable=True)(inp1)\n",
        "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x1)\n",
        "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x2)\n",
        "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x3)   \n",
        "    # x1 = Attention(maxlen)(x1)\n",
        "    # x2 = Attention(maxlen)(x2)\n",
        "    # x3 = Attention(maxlen)(x3)\n",
        "    # x=  Concatenate()([x1,x2,x3])\n",
        "    x1 = CuDNNLSTM(200, return_sequences=True)(x1)   \n",
        "    x1 = SeqSelfAttention(kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                    bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                    attention_regularizer_weight=1e-4,\n",
        "                    name='Attention')(x1) \n",
        "    x2=  GlobalMaxPooling1D()(x1)\n",
        "    x3= GlobalAveragePooling1D()(x1)\n",
        "    x=  Concatenate()([x2,x3])\n",
        "    x = Dropout(0.1)(x)\n",
        "    #x = Attention(maxlen)(x)\n",
        "    # layer = Dense(600,name='FC1')(x)\n",
        "    # layer = Dense(300,activation='relu')(layer)\n",
        "    layer = Dense(128,activation='relu')(x)\n",
        " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
        "    #layer = Activation('relu')(layer)\n",
        "    #layer = Dropout(0.4)(layer)\n",
        "    layer = Dense(64,name='FC2')(layer)\n",
        "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.4)(layer)\n",
        "   # layer=  Concatenate()([layer,inp2])\n",
        "    # layer=Dense(256,activation='relu')(layer)\n",
        "    # layer=Dense(128,activation='relu')(layer)\n",
        "    layer = Dense(1,name='out_layer',activation='sigmoid')(layer)\n",
        "\n",
        "    model = Model(inputs=[inp1],outputs=layer)\n",
        "\n",
        "    return model\n",
        "model_bi=BidLstm(max_len,max_features=max_words,embed_size=400)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL6PZy2EEUA8",
        "colab_type": "code",
        "outputId": "79e8da73-36ee-4484-8c2c-9396b7b0c327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "model_bi.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc',km.f1_score()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETsV7dEbOvX2",
        "colab_type": "code",
        "outputId": "02274b51-ff94-4e3b-e69b-94e13d3a2c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tok.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1073662"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc0kdfJjHFVM",
        "colab_type": "code",
        "outputId": "b848cf8f-1123-4e81-e95e-599ff9b64e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "model_bi.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 25, 400)      40000400    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)        (None, 25, 200)      481600      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Attention (SeqSelfAttention)    (None, 25, 200)      12865       cu_dnnlstm_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 200)          0           Attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 200)          0           Attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 400)          0           global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_average_pooling1d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 400)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          51328       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "FC2 (Dense)                     (None, 64)           8256        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64)           0           FC2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 64)           0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "out_layer (Dense)               (None, 1)            65          dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 40,554,514\n",
            "Trainable params: 40,554,514\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ynv6PiHHMtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
        "class_weights=dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRD3y4q4HSx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_filepath='/content/drive/My Drive/offenseval/'+'checkpoints/lstm_model_2020a_big_attention.h5'\n",
        "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
        "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=2,restore_best_weights=True)\n",
        "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMVJ0JriHqYz",
        "colab_type": "code",
        "outputId": "630fb472-e912-47ee-ca89-08dce0c6894e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "model_bi.fit([sequences_matrix_train],y_train,validation_data=([sequences_matrix_dev],y_test),epochs=10,batch_size=512,class_weight=class_weights,callbacks=[es,cp_check_point])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6956271 samples, validate on 1739068 samples\n",
            "Epoch 1/10\n",
            "6956271/6956271 [==============================] - 463s 66us/step - loss: 0.1194 - acc: 0.9558 - f1_score: 0.8734 - val_loss: 0.1136 - val_acc: 0.9570 - val_f1_score: 0.8761\n",
            "Epoch 2/10\n",
            "6956271/6956271 [==============================] - 462s 66us/step - loss: 0.1041 - acc: 0.9614 - f1_score: 0.8884 - val_loss: 0.1171 - val_acc: 0.9553 - val_f1_score: 0.8723\n",
            "Epoch 3/10\n",
            "6956271/6956271 [==============================] - 462s 66us/step - loss: 0.0904 - acc: 0.9663 - f1_score: 0.9018 - val_loss: 0.1130 - val_acc: 0.9588 - val_f1_score: 0.8801\n",
            "Epoch 4/10\n",
            "6956271/6956271 [==============================] - 462s 66us/step - loss: 0.0779 - acc: 0.9709 - f1_score: 0.9144 - val_loss: 0.1321 - val_acc: 0.9519 - val_f1_score: 0.8633\n",
            "Epoch 5/10\n",
            "6956271/6956271 [==============================] - 462s 66us/step - loss: 0.0678 - acc: 0.9745 - f1_score: 0.9244 - val_loss: 0.1306 - val_acc: 0.9562 - val_f1_score: 0.8728\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16e2a3e978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CFqhVzNHyFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_dl=model_bi.predict(sequences_matrix_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l72FqsLDhNSr",
        "colab_type": "code",
        "outputId": "13222eee-5564-4df7-eeea-f5c14668273e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#y_pred = model_bi.predict(X_dev, batch_size=30, verbose=1)\n",
        "\n",
        "print(classification_report(y_test, y_preds_dl.round()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98   1462602\n",
            "           1       0.82      0.95      0.88    276466\n",
            "\n",
            "    accuracy                           0.96   1739068\n",
            "   macro avg       0.90      0.96      0.93   1739068\n",
            "weighted avg       0.96      0.96      0.96   1739068\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FZE-gv7hPyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}