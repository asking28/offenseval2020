{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "offens_task3_bilstm_attention.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8urmVSlSm02S",
        "colab_type": "code",
        "outputId": "1353cbf1-3a60-46d2-a989-675197e5c0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9x5GAv2m-n6",
        "colab_type": "code",
        "outputId": "d9fab16c-3f13-42af-b6ab-5b2d30976994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import io\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Conv1D, Conv2D\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
        "from keras.layers import average\n",
        "import tensorflow_hub as hub\n",
        "from keras.layers import Average\n",
        "from keras.layers import Concatenate\n",
        "nltk.download('punkt')\n",
        "from numpy import random\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "from keras.layers import SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "import os\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "#plt.switch_backend('agg')\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzhRFUFJoARV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "9e34e078-6aed-497e-a962-ce043e9fdff7"
      },
      "source": [
        "!pip install keras_metrics\n",
        "!pip install emoji\n",
        "import keras_metrics as km"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_metrics\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.17.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n",
            "Installing collected packages: keras-metrics\n",
            "Successfully installed keras-metrics-1.1.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=3229cde818c6c3048ab1e02d9afb97083bb0ad8502f5b0c732cf5e3a83053cfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNryX5AgoDQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path='/content/drive/My Drive/offenseval/2020'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4KH3MBhoGJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UeuSDrPoLHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = root_path+\"/OffenseEval2020Data/task_c_distant.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oI5wHuDoUE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with ZipFile(file_name,'r') as zip:\n",
        "#   zip.printdir()\n",
        "#   zip.extractall(root_path,pwd=b'sem2020-t12')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oN0xag04JuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAB16jAwogy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=pd.read_csv('/content/drive/My Drive/offenseval/2020/task_c_distant_ann.tsv',delimiter='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wasVZPaqZTc",
        "colab_type": "code",
        "outputId": "c138214e-9c57-4dc5-f7ef-62feb0cb5d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>average_ind</th>\n",
              "      <th>average_grp</th>\n",
              "      <th>average_oth</th>\n",
              "      <th>std_ind</th>\n",
              "      <th>std_grp</th>\n",
              "      <th>std_oth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1159533712079503361</td>\n",
              "      <td>@USER Trump is a fucking idiot his dementia is...</td>\n",
              "      <td>0.833432</td>\n",
              "      <td>0.076110</td>\n",
              "      <td>0.107765</td>\n",
              "      <td>0.208334</td>\n",
              "      <td>0.098937</td>\n",
              "      <td>0.138649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1159533713044234241</td>\n",
              "      <td>@USER HELL YES! His grinned and thumbs up are ...</td>\n",
              "      <td>0.481062</td>\n",
              "      <td>0.367363</td>\n",
              "      <td>0.138841</td>\n",
              "      <td>0.345225</td>\n",
              "      <td>0.335924</td>\n",
              "      <td>0.083230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1159533718345830400</td>\n",
              "      <td>@USER Can't wait to see the shit show his deat...</td>\n",
              "      <td>0.438813</td>\n",
              "      <td>0.268574</td>\n",
              "      <td>0.377573</td>\n",
              "      <td>0.182609</td>\n",
              "      <td>0.186880</td>\n",
              "      <td>0.254621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1159533739871002625</td>\n",
              "      <td>@USER @USER @USER This guys is dumb check his ...</td>\n",
              "      <td>0.712995</td>\n",
              "      <td>0.123504</td>\n",
              "      <td>0.111130</td>\n",
              "      <td>0.248839</td>\n",
              "      <td>0.107572</td>\n",
              "      <td>0.067552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1159533742366633984</td>\n",
              "      <td>@USER @USER Fuck him better than his hoes</td>\n",
              "      <td>0.691414</td>\n",
              "      <td>0.146723</td>\n",
              "      <td>0.192282</td>\n",
              "      <td>0.204415</td>\n",
              "      <td>0.154818</td>\n",
              "      <td>0.104436</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id  ...   std_oth\n",
              "0  1159533712079503361  ...  0.138649\n",
              "1  1159533713044234241  ...  0.083230\n",
              "2  1159533718345830400  ...  0.254621\n",
              "3  1159533739871002625  ...  0.067552\n",
              "4  1159533742366633984  ...  0.104436\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf3ey4KDqjPa",
        "colab_type": "text"
      },
      "source": [
        "## Twitter Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUZMDz7bqbj6",
        "colab_type": "code",
        "outputId": "db52c50a-ab44-4fa1-9b44-aad0190060f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Sentimix/word2vec_twitter_tokens.bin', binary=True,unicode_errors='ignore')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaAQ6sepqoFR",
        "colab_type": "code",
        "outputId": "4c2714b9-2871-42d2-e87d-a8c69b087e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model['fuck'].shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSzr_yTlquRV",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV9PIibOqqwJ",
        "colab_type": "code",
        "outputId": "5642168a-1f26-4954-81d2-403d770dbaa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install tqdm\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtzr1Plqy_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_pattern(input_txt, pattern,with_space=False):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    if with_space==False:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    else:\n",
        "      for i in r:\n",
        "        input_txt = re.sub(i, ' ', input_txt)\n",
        "    return input_txt \n",
        "\n",
        "import emoji\n",
        "import pickle\n",
        "import re\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/contractions.pkl','rb')as f:\n",
        "  contractions=pickle.load(f)\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "contractions=Counter(contractions)\n",
        "with open('/content/drive/My Drive/Sentimix/helper_data/acronyms.pkl','rb')as f:\n",
        "  acronyms=pickle.load(f)\n",
        "acronyms=Counter(acronyms)\n",
        "def acronym(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if acronyms[word]!=0:\n",
        "        w_l.append(acronyms[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "with open('/content/drive/My Drive/Sentimix/hinglish_to_english.pickle','rb')as f:\n",
        "  hing_to_eng=pickle.load(f)\n",
        "hing_to_eng=Counter(hing_to_eng)\n",
        "def hindi_se_english(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    w_l=[]\n",
        "    sent=str(df[column][i])\n",
        "    for word in sent.split():\n",
        "      if hing_to_eng[word]!=0:\n",
        "        w_l.append(hing_to_eng[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
        "    cuss_dict=pickle.load(handle)\n",
        "cuss_dict=Counter(cuss_dict)\n",
        "def replace_cuss(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if cuss_dict[word]!=0:\n",
        "        w_l.append('abuse')\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_contraction(df,column):\n",
        "  s_l=[]\n",
        "  for i in tqdm(range(df.shape[0])):\n",
        "    sent=str(df[column][i]).lower()\n",
        "    w_l=[]\n",
        "    for word in sent.split():\n",
        "      if contractions[word]!=0:\n",
        "        w_l.append(contractions[word])\n",
        "      else:\n",
        "        w_l.append(word)\n",
        "    s_l.append(' '.join(w_l))\n",
        "  return s_l\n",
        "def remove_pattern_rep(input_txt, pattern,rep_pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "      input_txt = re.sub(i, rep_pattern, input_txt)\n",
        "\n",
        "    return input_txt \n",
        "def cleaning(data_f,cleaning_col,new_col):\n",
        "  data_f.reset_index(drop=True,inplace=True)\n",
        "  for i in tqdm(range(data_f.shape[0])):\n",
        "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
        "  # #data_f[cleaning_col]=replace_cuss(data_f,cleaning_col)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[cleaning_col],\"_\",with_space=True)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
        "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
        "  # data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@[\\w]*\",\"<USR>\")\n",
        "  # data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"http\\S+\",\"<URL>\")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[cleaning_col], \"[0-9]+\",\"<NUM>\")\n",
        "  #data_f[new_col]=hindi_se_english(data_f,cleaning_col)\n",
        "  data_f[new_col]=remove_contraction(data_f,new_col)\n",
        "  data_f[new_col]=acronym(data_f,new_col)\n",
        "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
        "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
        "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
        "  #data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
        "  #data_f.drop(['text'],inplace=True)\n",
        "  return data_f\n",
        "import numpy as np\n",
        "#a=cleaning(df_chunky[0],'text','clean_col')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LGDHrflq20M",
        "colab_type": "code",
        "outputId": "7e60eb55-5786-4d99-947e-c4893efa1810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "_=cleaning(train_data,'text','clean_col')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/188973 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "100%|██████████| 188973/188973 [04:03<00:00, 777.53it/s]\n",
            "100%|██████████| 188973/188973 [00:04<00:00, 43565.45it/s]\n",
            "100%|██████████| 188973/188973 [00:04<00:00, 42749.55it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvmtMOXorBH7",
        "colab_type": "code",
        "outputId": "c94cad82-fca6-49b3-cbae-bedf58dff83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "train_data.columns"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'text', 'average_ind', 'average_grp', 'average_oth', 'std_ind',\n",
              "       'std_grp', 'std_oth', 'clean_col'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ2RXd8KbobC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_=pd.DataFrame({'text':train_data['clean_col'],'average_ind':train_data['average_ind'],'average_oth':train_data['average_oth'],'average_grp':train_data['average_grp']})\n",
        "# df_.to_csv(root_path+'/task_c_cleaned.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkHnhdPVxHkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_temp=pd.DataFrame({'text':train_data['clean_col'],'label':train_data['average']})\n",
        "# df_temp.to_csv('/content/drive/My Drive/offenseval/2020/tas_b_2020.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF6uN5Njsjp8",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Train Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR5o7mU-sdU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc3_yfXCsn4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train,df_test=train_test_split(train_data,test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjvGZ5Gtssk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_words =10000\n",
        "max_len = 25\n",
        "tok = Tokenizer(max_words)\n",
        "tok.fit_on_texts(train_data['clean_col'].astype(str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uiq2b0ZPsz1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_train = tok.texts_to_sequences(df_train['clean_col'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WUXbJyfs6IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_dev = tok.texts_to_sequences(df_test['clean_col'].astype(str))\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxPk6kDK-4Ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=df_train[['average_ind','average_grp','average_oth']].to_numpy().argmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNUat-VkdpQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test=df_test[['average_ind','average_grp','average_oth']].to_numpy().argmax(axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmwGTwGw_2Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df_train[['average_ind','average_grp','average_oth']].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s4k9bqks_OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_train=(df_train['average']>=0.5).astype(int)\n",
        "# y_test=(df_test['average']>0.5).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL6UCtGYtHwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Activation\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "def custom_gelu(x):\n",
        "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
        "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UgxNXypd3-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhGwoIgieYkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "47d0ff65-66c1-44b6-bf49-c7bea9ccc45f"
      },
      "source": [
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(151178,)\n",
            "(37795,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-CGTzr4tmeH",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "581UbiUntv1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join('/content/drive/My Drive/IR_project/glove.6B', 'glove.6B.300d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUCAVslAtlOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index=tok.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-brG0wAts8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix_1 = np.zeros([max_words + 1, 400])\n",
        "for word, i in tok.word_index.items():\n",
        "    if word in model:\n",
        "      if i>max_words:\n",
        "        break\n",
        "      embedding_matrix_1[i] = model[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKp2BKRmtzus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Concatenate\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqMpyQTat4Ss",
        "colab_type": "code",
        "outputId": "e08d6e56-8d69-4fd3-b796-d7e87f21d16f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "!pip install keras-self-attention"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/44/3e/eb1a7c7545eede073ceda2f5d78442b6cad33b5b750d7f0742866907c34b/keras-self-attention-0.42.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.17.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.42.0-cp36-none-any.whl size=17296 sha256=ea50ac15605552f0e327f8525eb8b6afea07064e9aa91e5d31e34423d6d9fe4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/05/a0/99c0cf60d383f0494e10eca2b238ea98faca9a1fe03cac2894\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zNjzoa2t6y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
        "from sklearn.utils import class_weight\n",
        "class Attention(Layer):\n",
        "    def __init__(self,step_dim=20,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, Dropout\n",
        "\n",
        "#max_len=\n",
        "\n",
        "def BidLstm(maxlen, max_features, embed_size):\n",
        "    inp1 = Input(shape=(maxlen, ))\n",
        "    #inp2=Input(shape=(1,))\n",
        "    #x=Embedding(len(word_index)+1,embed_size)(inp1)\n",
        "    x1 = Embedding(max_words+1,embed_size,weights=[embedding_matrix_1],\n",
        "                  trainable=True)(inp1)\n",
        "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
        "    #                trainable=True)(inp1)\n",
        "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x1)\n",
        "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x2)\n",
        "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
        "    #                        recurrent_dropout=0.4))(x3)   \n",
        "    # x1 = Attention(maxlen)(x1)\n",
        "    # x2 = Attention(maxlen)(x2)\n",
        "    # x3 = Attention(maxlen)(x3)\n",
        "    # x=  Concatenate()([x1,x2,x3])\n",
        "    x1 = Bidirectional(CuDNNLSTM(200, return_sequences=True))(x1)   \n",
        "    x1 = SeqSelfAttention(kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                    bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                    attention_regularizer_weight=1e-4,\n",
        "                    name='Attention')(x1) \n",
        "    x2=  GlobalMaxPooling1D()(x1)\n",
        "    x3= GlobalAveragePooling1D()(x1)\n",
        "    x=  Concatenate()([x2,x3])\n",
        "    x = Dropout(0.1)(x)\n",
        "    #x = Attention(maxlen)(x)\n",
        "    # layer = Dense(600,name='FC1')(x)\n",
        "    # layer = Dense(300,activation='relu')(layer)\n",
        "    layer = Dense(128,activation='relu')(x)\n",
        " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
        "    #layer = Activation('relu')(layer)\n",
        "    #layer = Dropout(0.4)(layer)\n",
        "    layer = Dense(64,name='FC2')(layer)\n",
        "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.4)(layer)\n",
        "   # layer=  Concatenate()([layer,inp2])\n",
        "    # layer=Dense(256,activation='relu')(layer)\n",
        "    # layer=Dense(128,activation='relu')(layer)\n",
        "    layer = Dense(3,name='out_layer',activation='softmax')(layer)\n",
        "\n",
        "    model = Model(inputs=[inp1],outputs=layer)\n",
        "\n",
        "    return model\n",
        "model_bi=BidLstm(max_len,max_features=max_words,embed_size=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NViEAJS9t9lX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_bi.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['acc',km.f1_score()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT_1ZJX7uAH9",
        "colab_type": "code",
        "outputId": "1b02018c-0422-41e9-d734-bdf943268b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "model_bi.summary()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 25)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 25, 400)      4000400     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 25, 400)      963200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Attention (SeqSelfAttention)    (None, 25, 400)      25665       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 400)          0           Attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 400)          0           Attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 800)          0           global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_average_pooling1d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 800)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128)          102528      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "FC2 (Dense)                     (None, 64)           8256        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 64)           0           FC2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 64)           0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "out_layer (Dense)               (None, 3)            195         dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,100,244\n",
            "Trainable params: 5,100,244\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv3iu0fjuCZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
        "class_weights=dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmuQzEZUuEsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cp_filepath='/content/drive/My Drive/offenseval/'+'checkpoints/lstm_model_2020b_big_attention.h5'\n",
        "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
        "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=2,restore_best_weights=True)\n",
        "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bsiPzmFuKtE",
        "colab_type": "code",
        "outputId": "ac13162a-bb72-472b-9a30-4ff090cdbfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "model_bi.fit([sequences_matrix_train],y_train,validation_data=([sequences_matrix_dev],y_test),epochs=10,batch_size=1024,class_weight=class_weights,callbacks=[es,cp_check_point])\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 151178 samples, validate on 37795 samples\n",
            "Epoch 1/10\n",
            "151178/151178 [==============================] - 9s 61us/step - loss: 0.5544 - acc: 0.7743 - f1_score: 0.0350 - val_loss: 0.4707 - val_acc: 0.8030 - val_f1_score: 0.0127\n",
            "Epoch 2/10\n",
            "151178/151178 [==============================] - 8s 55us/step - loss: 0.3631 - acc: 0.8554 - f1_score: 0.0204 - val_loss: 0.3186 - val_acc: 0.8695 - val_f1_score: 0.0316\n",
            "Epoch 3/10\n",
            "151178/151178 [==============================] - 8s 55us/step - loss: 0.3014 - acc: 0.8761 - f1_score: 0.0161 - val_loss: 0.3664 - val_acc: 0.8515 - val_f1_score: 0.0283\n",
            "Epoch 4/10\n",
            "151178/151178 [==============================] - 8s 55us/step - loss: 0.2659 - acc: 0.8864 - f1_score: 0.0132 - val_loss: 0.3229 - val_acc: 0.8696 - val_f1_score: 0.0359\n",
            "Epoch 5/10\n",
            "151178/151178 [==============================] - 8s 55us/step - loss: 0.2359 - acc: 0.8960 - f1_score: 0.0114 - val_loss: 0.3023 - val_acc: 0.8789 - val_f1_score: 0.0430\n",
            "Epoch 6/10\n",
            "151178/151178 [==============================] - 8s 55us/step - loss: 0.2105 - acc: 0.9040 - f1_score: 0.0100 - val_loss: 0.3323 - val_acc: 0.8675 - val_f1_score: 0.0388\n",
            "Epoch 7/10\n",
            "151178/151178 [==============================] - 8s 55us/step - loss: 0.1869 - acc: 0.9135 - f1_score: 0.0086 - val_loss: 0.3922 - val_acc: 0.8695 - val_f1_score: 0.0426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f5016d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn1nq_mLuSg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_preds_dl=model_bi.predict(sequences_matrix_dev)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMPo73aNhpAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "798ef234-fc46-4f4a-f644-83294b09a614"
      },
      "source": [
        "y_preds_dl"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.82520401e-01, 1.09897470e-02, 6.48984080e-03],\n",
              "       [9.99997854e-01, 2.12838199e-06, 1.73419661e-08],\n",
              "       [9.99440491e-01, 4.25356207e-04, 1.34196773e-04],\n",
              "       ...,\n",
              "       [9.99999404e-01, 5.71469855e-07, 1.05636069e-08],\n",
              "       [9.96202528e-01, 3.68329603e-03, 1.14121096e-04],\n",
              "       [9.99391675e-01, 4.09951987e-04, 1.98411479e-04]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNLgNPTRyA3R",
        "colab_type": "code",
        "outputId": "78d0427e-2a27-416b-f208-a0db1d1cd2e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#y_pred = model_bi.predict(X_dev, batch_size=30, verbose=1)\n",
        "\n",
        "print(classification_report(y_test, y_preds_dl.argmax(axis=-1)))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.89      0.93     30598\n",
            "           1       0.71      0.86      0.78      4867\n",
            "           2       0.44      0.78      0.56      2330\n",
            "\n",
            "    accuracy                           0.88     37795\n",
            "   macro avg       0.71      0.84      0.76     37795\n",
            "weighted avg       0.91      0.88      0.89     37795\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uldsIGhJyBVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}